{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License Â© 2021 Lorena A. Barba, Tingyu Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression\n",
    "\n",
    "Welcome to Lesson 3 of our _Engineering Computations_ module on deep learning!\n",
    "\n",
    "So far, we have only modeled the relationship between one input variable (also called _feature_) $x$ and one output variable $y$. More often than not, real-world model fitting involves making predictions using more than one feature. For example, you can build a model to predict the box-office gross revenue of Hollywood movies using trailer views, Wikipedia page views, critic ratings and time of release; or to predict the annual energy consumption of a building using its occupancy, structural information, weather data and so on.  In this lesson, we are going to extend the linear regression model to multiple input variables, i.e., we explore **multiple linear regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real data set to work with\n",
    "\n",
    "To have some data to work with, we grabbed the [auto miles per gallon (MPG) dataset](http://archive.ics.uci.edu/ml/datasets/Auto+MPG) from the UCI Machine Learning Repository, removed the missing data and formatted it as a csv file. Our goal is to predict the MPG (fuel efficiency) of a car using its technical specs.\n",
    "\n",
    "Let's begin by importing the Python libraries we will use. The [Seaborn](https://seaborn.pydata.org) library for statistical data visualization will help us make beautiful plots. Seaborn is built on top of Matplotlib, and its plotting functions work seamlessly with dataframes or arrays.\n",
    "\n",
    "##### Note:\n",
    "> From now on, we choose the import statements with a short alias. We didn't choose that in the earlier modules of the _Engineering Computations_ series, because we wanted to be more explicit and give you more readable code. But it is the custom in Python programming to use the shorthand versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the data and take a peek at it.\n",
    "If you need to download the data, execute the following in a code cell:\n",
    "\n",
    "```Python\n",
    "from urllib.request import urlretrieve\n",
    "URL = 'https://go.gwu.edu/engcomp6data3'\n",
    "urlretrieve(URL, 'auto_mpg.csv')\n",
    "```\n",
    "\n",
    "In that case, the file will be downloaded in your working directory, so you should remove the directory path `../data/` from the file string below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data = pd.read_csv('../data/auto_mpg.csv')\n",
    "mpg_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first few rows you get a sense for what's in this data set. Use [`pandas.DataFrame.info()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html) to see a quick summary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 392 entries and each is associated with a specific car model. The data consists of altogether 9 columns. Except for `car name`, values in all other columns are numeric. Despite being numeric, the `origin` column, indicating the country of origin, corresponds to categorical data.  We expect that the name of a car won't affect its MPG (country of origin might); for simplicity, we exclude `car name` and `origin` as features to predict the MPG.\n",
    "\n",
    "Now, let's define the feature columns: `x_cols` and the output column: `y_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'mpg'\n",
    "x_cols = mpg_data.columns.drop(['car name', 'origin', 'mpg'])  # also drop mpg column\n",
    "\n",
    "print(x_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up keeping 6 features, or independent variables (all technical specs of a car), that we expect to be correlated with MPG, the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Before choosing a model to fit our data, exploring the data is equally important but often ignored. The best way to start is to visualize the relationship between input and output variables.\n",
    "\n",
    "We have used scatter plots before to visualize the relationship between just two variables. \n",
    "Since now we are dealing with 6 independent variables, we want to make such a plot for each one; luckily, `seaborn` provides the handy function [`seaborn.pairplot()`](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to plot these 6 figures in one go. Check it out! You can double-click the figure to expand the view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=mpg_data, height=5, aspect=1,\n",
    "             x_vars=x_cols,\n",
    "             y_vars=y_col);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect these plots and make some mental notes of what you see. \n",
    "The features: `model_year`, `acceleration` show a positive correlation with `mpg`, while the rest show a negative correlation with `mpg`.\n",
    "It looks like a linear model might represent well the relationship of all six features with our output variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model in matrix form\n",
    "\n",
    "If every feature $x_i$ is correlated with $y$ individually, it is natural to think that combining them linearly would be a good fit for $y$. Formally, the multiple linear regression model for $d$ input variables can be written as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_d x_d, \n",
    "$$\n",
    "\n",
    "where the \"hat\" on $y$ denotes a predicted value.\n",
    "Notice that we have $d+1$ weights for $d$ features, and $w_0$ is the intercept term. By letting $x_0 = 1$ for all data points, we can simplify the notation as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sum_{i=0}^{d} w_d x_d = \\mathbf{w}^T \\mathbf{x}, \n",
    "$$\n",
    "\n",
    "where $\\mathbf{w} = (w_0, w_1, \\ldots, w_d)^T$ is the vector of weights, and $\\mathbf{x} = (x_0, x_1, \\ldots, x_d)^T$ the vector of input variables.\n",
    "\n",
    "Since we've used subscripts to denote features, let's index our dataset entries with superscripts. For example, $x_1^{(i)}$ represents the `cylinders` (the first feature) value of the $i$-th car model.\n",
    "\n",
    "Suppose our dataset has $N$ entries; writing out our model for each entry, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y}^{(1)} & = w_0 x_0^{(1)} + w_1 x_1^{(1)} + w_2 x_2^{(1)} + \\ldots + w_d x_d^{(1)} \\\\\n",
    "\\hat{y}^{(2)} & = w_0 x_0^{(2)} + w_1 x_1^{(2)} + w_2 x_2^{(2)} + \\ldots + w_d x_d^{(2)} \\\\\n",
    "&\\vdots \\\\\n",
    "\\hat{y}^{(N)} & = w_0 x_0^{(N)} + w_1 x_1^{(N)} + w_2 x_2^{(N)} + \\ldots + w_d x_d^{(N)}  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Finally, we arrive at the matrix form of the multiple linear regression model:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = X\\mathbf{w}\n",
    "$$\n",
    "\n",
    "The $X$ is the matrix of our input variables. To form $X$, we need to pad a column of $1$s to the left of our original data as the dummy feature corresponding to the intercept $w_0$. We use $\\hat{\\mathbf{y}}$ to represent the vector of the predicted output variables, and $\\mathbf{y}$ to represent the vector of the observed (true) output variables.\n",
    "\n",
    "Before coding our model, let's import from the automatic differentiation library `autograd` both its NumPy wrapper and the `grad()` function, as we learned to do in Lesson 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the input matrix $X$ and the vector $\\mathbf{y}$ directly from our dataset.\n",
    "In the code below, we grab the columns `x_cols` from the dataframe `mpg_data` and extract the values into a NumPy array `X` (a matrix). \n",
    "The NumPy function [`hstack()`](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html) is used to stack arrays horizontally (by columns). \n",
    "We also slice the single column `y_col` of the dataframe, and extract the values into a NumPy array `y`. \n",
    "Check out the array shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mpg_data[x_cols].values\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))  # pad 1s to the left of input matrix\n",
    "y = mpg_data[y_col].values\n",
    "\n",
    "print(f\"{X.shape = }, {y.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the single-variable linear regression model of Lesson 1, we use the the **mean squared error** loss function, over all the data points: \n",
    "\n",
    "$$L(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{N} \\sum_{i=0}^{N}(y - \\hat{y}^{(i)})^2$$\n",
    "\n",
    "We're ready to define Python functions for the multiple linear regresssion model and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(params, X):\n",
    "    '''\n",
    "    The linear regression model in matrix form.\n",
    "    Arguments:\n",
    "      params: 1D array of weights for the linear model\n",
    "      X     : 2D array of input values\n",
    "    Returns:\n",
    "      1D array of predicted values\n",
    "    '''\n",
    "    return np.dot(X, params)\n",
    "\n",
    "def mse_loss(params, model, X, y):\n",
    "    '''\n",
    "    The mean squared error loss function.\n",
    "    Arguments:\n",
    "      params: 1D array of weights for the linear model\n",
    "      model : function for the linear regression model\n",
    "      X     : 2D array of input values\n",
    "      y     : 1D array of predicted values\n",
    "    Returns:\n",
    "      float, mean squared error\n",
    "    '''\n",
    "    y_pred = model(params, X)\n",
    "    return np.mean( np.sum((y-y_pred)**2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the weights by gradient descent\n",
    "\n",
    "Following Lesson 2, we know that `autograd.grad()` will give us the function to compute the derivatives required in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = grad(mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function with a random initial guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(np.random.rand(X.shape[1]), linear_regression, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, it does not look nice. With the random weights, the gradient values are huge. Let us try with a few iterations in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 30\n",
    "alpha = 0.001\n",
    "params = np.zeros(X.shape[1])\n",
    "\n",
    "for i in range(max_iter):\n",
    "    descent = gradient(params, linear_regression, X, y)\n",
    "    params = params - descent * alpha\n",
    "    loss = mse_loss(params, linear_regression, X, y)\n",
    "    if i%5 == 0:\n",
    "        print(f\"iteration {i:3}, {loss = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! This is definitely not good. What could be going on here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "\n",
    "As you saw above, the gradient descent algorithm is blowing up. This is because of the big numbers in certain columns, for instance, look at the `weight` column. In addition, having features with varying magnitudes will also lead to slow convergence in the gradient descent iterations. Therefore, it is critical to make sure that all features are on a similar scale. This step is also called **feature scaling** or **data normalization**.\n",
    "\n",
    "Let's check the range of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data[x_cols].describe().loc[['max', 'min']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One commonly used feature scaling technique is **min-max scaling**, which scales the range of each feature to the interval $[0,1]$. If $x$ is the original value of a feature, its scaled (normalized) value $x^{\\prime}$ is given as:\n",
    "\n",
    "$$\n",
    "x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)}\n",
    "$$\n",
    "\n",
    "We will now introduce a new Python library: [**scikit-learn**](https://scikit-learn.org/stable/). \n",
    "It is the standard tool for machine-learning tasks in Python. \n",
    "So far, we've made some headway with the tools you know from previous _Engineering Computations_ modules, including NumPy, SymPy, and pandas. \n",
    "But we reached a point where it's so much easier to start using `scikit-learn`!\n",
    "\n",
    "Here, we'll use the function [`sklearn.preprocessing.MinMaxScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to rescale our $X$.\n",
    "\n",
    "And check the range of each column of $X$ again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(mpg_data[x_cols])\n",
    "X_scaled = np.hstack((np.ones((X_scaled.shape[0], 1)), X_scaled))    # add the column for intercept\n",
    "\n",
    "pd.DataFrame(X_scaled).describe().loc[['max', 'min']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that column **0** corresponds to the dummy data for the intercept. All values in that column are 1.\n",
    "\n",
    "Finally, we are ready to run gradient descent to find the optimal parameters for our multiple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "alpha = 0.001\n",
    "params = np.zeros(X.shape[1])\n",
    "\n",
    "for i in range(max_iter):\n",
    "    descent = gradient(params, linear_regression, X_scaled, y)\n",
    "    params = params - descent * alpha\n",
    "    loss = mse_loss(params, linear_regression, X_scaled, y)\n",
    "    if i%100 == 0:\n",
    "        print(f\"iteration {i:3}, {loss = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the trained weights. Recall that the first element is the intercept, and the rest correspond to the 6 features respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make predictions with our model, and this step reduces to a matrix-vector multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gd = X_scaled @ params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How accurate is the model?\n",
    "\n",
    "One thing that we haven't discussed till now is how to quantify the accuracy of a model. For regression problems, two basic metrics are the mean absolute error (MAE) and the root-mean-squared error (RMSE). The latter is just the square root of the MSE loss function that we used above.\n",
    "\n",
    "$$\n",
    "\\mathrm{MAE}(\\mathbf{y}, \\hat{\\mathbf{y}})=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y^{(i)}-\\hat{y}^{(i)}\\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{RMSE}(\\mathbf{y}, \\hat{\\mathbf{y}})=\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "Most common metrics are available in **scikit-learn**. Let's compute both errors using the corresponding functions in the [`sklearn.metrics`](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(y, y_pred_gd)\n",
    "rmse = mean_squared_error(y, y_pred_gd, squared=False)\n",
    "print(f\"gradient descent\")\n",
    "print(f\"{mae  = }\")\n",
    "print(f\"{rmse = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with scikit-learn\n",
    "\n",
    "We want to mention that the [`LinearRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression#sklearn-linear-model-linearregression) function in **scikit-learn** offers the same capability we've coded from scratch above. Now with a better understanding of the model, you should feel more comfortable to use these black-boxes. \n",
    "\n",
    "Check out how the code looks like using this tool: just four lines of code do all the work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=False).fit(X, y)\n",
    "print(\"params:\")\n",
    "print()\n",
    "\n",
    "y_pred_sklearn = model.predict(X)\n",
    "\n",
    "mae = mean_absolute_error(y, y_pred_sklearn)\n",
    "rmse = mean_squared_error(y, y_pred_sklearn, squared=False)\n",
    "print(f\"scikit-learn linear regression\")\n",
    "print(f\"{mae  = }\")\n",
    "print(f\"{rmse = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with pseudo-inverse\n",
    "\n",
    "We want to conclude this notebook with a callback to the final lesson in the _Engineering Computations_ module on Linear Algebra, [Module 4](https://openedx.seas.gwu.edu/courses/course-v1:GW+EngComp4+2019/about). Recall that we can use singular value decomposition (SVD) to obtain the pseudo-inverse of a matrix and that the pseudo-inverse offers a least-squares solution of the corresponding linear system. Given $X$ and $\\mathbf{y}$, finding the linear regression weights $\\mathbf{w}$ that minimize the MSE loss function is exactly a least-squares problem.\n",
    "\n",
    "Performing SVDs on large datasets might not be ideal, but let's try on this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import pinv\n",
    "\n",
    "params = pinv(X) @ y\n",
    "y_pred_svd = X @ params\n",
    "\n",
    "mae = mean_absolute_error(y, y_pred_svd)\n",
    "rmse = mean_squared_error(y, y_pred_svd, squared=False)\n",
    "print(f\"linear regression using pseudo inverse\")\n",
    "print(f\"{mae  = }\")\n",
    "print(f\"{rmse = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look carefully enough, you will notice that the error from using pseudo-inverse is almost identical to the error from using the `sklearn.linear_model.LinearRegression()` function. In fact, that is exactly how `LinearRegression()` is implemented in **scikit-learn**, since a closed-form solution is available. However, for more complicated models, we have to use gradient descent. \n",
    "\n",
    "And this concludes Lesson 3 of our _Engineering Computations_ module on deep learning. \n",
    "We take a step-by-step approach to help you build understanding and demistify this booming subject that every scientist and engineer should know about!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we've learned\n",
    "\n",
    "- The [Seaborn](https://seaborn.pydata.org/) library for statistical visualization has handy tools to make beautiful plots!\n",
    "- A linear regression model with many input variables (a.k.a. features) is written neatly in matrix form as $\\hat{\\mathbf{y}} = X\\mathbf{w}$, where $X$ is the matrix of features and $w$ is the vector of weights.\n",
    "- Gradient descent can blow up with features that have disparate scales. Feature scaling (or normallization) solves this problem.\n",
    "- [**scikit-learn**](https://scikit-learn.org/stable/) is the standard tool for machine-learning tasks in Python.\n",
    "- The [`LinearRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression#sklearn-linear-model-linearregression) function in **scikit-learn** fits a linear model with multiple features.\n",
    "- We can also do multiple linear regression using the matrix pseudo-inverse, obtained with SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
