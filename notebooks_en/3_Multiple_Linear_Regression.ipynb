{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License Â© 2021 Lorena A. Barba, Tingyu Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression\n",
    "\n",
    "So far, we have only modeled the relationship between one input variable (or feature) $x$ and one output variable $y$. More often than not, real-world model fitting problems involve making predictions using more than one features. For example, you can predict the box office gross of Hollywood movies using trailer views, Wikipedia page views, critic ratings and time of release; or predict the annual energy consumption of a building using its occupancy, structural information, weather data and so on.  In this notebook, we are going to extend the linear regression model to multiple linear regression model, which can predict one output variable using multiple features.\n",
    "\n",
    "To have some data to work with, we grabbed the [auto miles per gallon (MPG) dataset](http://archive.ics.uci.edu/ml/datasets/Auto+MPG) from UCI Machine Learning Repository, removed the missing data and formatted it to csv. Our goal is to predict the MPG (fuel efficiency) of a car using its technical specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the data and take a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data = pd.read_csv('../data/auto_mpg.csv')\n",
    "mpg_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `pandas.DataFrame.info()` gives us a quick summary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 392 entries and each is associated with a specific car model. There are alltogether 9 columns. Except for `car name`, values in all other columns are numeric. The name of a car won't directly affect its MPG. Despite being numeric, the `origin` column, indicating the country of origin, should be categorical. Here for simplicity, we don't include `car name` and `origin` as the features to predict the MPG. Now, let's define the feature columns: `x_cols` and the output column: `y_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'mpg'\n",
    "x_cols = mpg_data.columns.drop(['car name', 'origin', 'mpg'])  # also drop mpg column\n",
    "\n",
    "print(x_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with keeping 6 features (all technical specs of a car) that are seemingly correlated with MPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Before choosing a model to fit our data, understanding the data is equally important but often ignored. The best way to start is to visualize the relationship between input and output variables.\n",
    "\n",
    "Remember we used to make a scatter plot when we only have one feature $x$ to observe the relationship. Since now we are dealing with 6 features, we want to make such plot for every $x$. The data visualization package `seaborn` provides us with a handy function [`seaborn.pairplot()`](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to plot these 6 figures in one go. Double-click the figure to expand the view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=mpg_data, height=5, aspect=1,\n",
    "             x_vars=x_cols,\n",
    "             y_vars=y_col);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features: `model_year`, `acceleration` show a positive correlation with `mpg`, while the rest show a negative correlation with `mpg`.\n",
    "All six features demonstrate more or less a linear relationship with our output variable. That's why using linear regression model suits this problem well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "If every feature is correlated with $y$ individually, it is natural to thinw combining them linearly would be a good fit for $y$. Formally, the multiple linear regression model for $d$ input variables can be written as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_d x_d, \n",
    "$$\n",
    "\n",
    "where the \"hat\" on $y$ denotes a predicted value.\n",
    "Notice that we have $d+1$ weights for $d$ features, and $w_0$ is the intercept term. By letting $x_0 = 1$ for all data points, we can simplify the notation as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sum_{i=0}^{d} w_d x_d = \\mathbf{w}^T \\mathbf{x}, \n",
    "$$\n",
    "\n",
    "where $\\mathbf{w} = (w_0, w_1, \\ldots, w_d)^T$ is the vector of weights, and $\\mathbf{x} = (x_0, x_1, \\ldots, x_d)^T$ the vector of input variables.\n",
    "\n",
    "Since we've used subscript to denote features, let's index our dataset entries with superscript. For example, $x_1^{(i)}$ represent the `cylinders` (the first feature) value of the $i$-th car model.\n",
    "\n",
    "Suppose our dataset has $N$ entries, writing out our model for each entry, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y}^{(1)} & = w_0 x_0^{(1)} + w_1 x_1^{(1)} + w_2 x_2^{(1)} + \\ldots + w_d x_d^{(1)} \\\\\n",
    "\\hat{y}^{(2)} & = w_0 x_0^{(2)} + w_1 x_1^{(2)} + w_2 x_2^{(2)} + \\ldots + w_d x_d^{(2)} \\\\\n",
    "&\\vdots \\\\\n",
    "\\hat{y}^{(N)} & = w_0 x_0^{(N)} + w_1 x_1^{(N)} + w_2 x_2^{(N)} + \\ldots + w_d x_d^{(N)}  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Finally, we arrive at the matrix form of the multiple linear regression model:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = X\\mathbf{w}\n",
    "$$\n",
    "\n",
    "The $X$ is the matrix of our input variables. To form $X$, we need to pad a column of $1$s to the left of our original data as the dummy feature corresponding to the intercept $w_0$. We use $\\hat{\\mathbf{y}}$ to represent the vector of the predicted output variables, use $\\mathbf{y}$ to represent the vector of the observed (true) output variables.\n",
    "\n",
    "Before coding our model, let's import the automatic differentiation library `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the input matrix $X$ and the vector $\\mathbf{y}$ directly from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mpg_data[x_cols].values\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))  # pad 1s to the left of input matrix\n",
    "y = mpg_data[y_col].values\n",
    "\n",
    "print(f\"{X.shape = }, {y.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the single-variable linear regression model in lesson 1, let's use the same loss function: $L(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{N} \\sum_{i=0}^{N}(y - \\hat{y}^{(i)})^2$. It is also called the **mean squared error** loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(params, X):\n",
    "    \"\"\"\n",
    "    The linear regression model in matrix form.\n",
    "    \"\"\"\n",
    "    return np.dot(X, params)\n",
    "\n",
    "def mse_loss(params, model, X, y):\n",
    "    \"\"\"\n",
    "    The mean squared error loss function.\n",
    "    \"\"\"\n",
    "    y_pred = model(params, X)\n",
    "    return np.mean( np.sum((y-y_pred)**2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Following lesson 2, we know that `autograd.grad()` would give us the function to compute the derivatives required in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = grad(mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let test the function with a random initial guess,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(np.random.rand(X.shape[1]), linear_regression, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, it does not look nice. With the random weights, the gradient values are huge. Let us try with a few iterations in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 30\n",
    "alpha = 0.001\n",
    "params = np.zeros(X.shape[1])\n",
    "\n",
    "for i in range(max_iter):\n",
    "    descent = gradient(params, linear_regression, X, y)\n",
    "    params = params - descent * alpha\n",
    "    loss = mse_loss(params, linear_regression, X, y)\n",
    "    if i%5 == 0:\n",
    "        print(f\"iteration {i:3}, {loss = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these weights, you have to choose a very tiny learning rate so that it won't blow up during gradient descent. This is because of the big numbers in certain columns, for instance, the `weight` column. In addition, having features with varying magnitudes will also lead to a slow convergence in the gradient descent. Therefore, it is critical to make sure that all features are on a similar scale. This step is also called **feature scaling** or **data normalization**.\n",
    "\n",
    "Let's check the range of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_data[x_cols].describe().loc[['max', 'min']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One commonly used feature scaling technique is min-max scaling, which scales the range of features in $[0,1]$. If $x$ is the original value of a feature, its scaled (normalized) value $x^{\\prime}$ is given as:\n",
    "\n",
    "$$\n",
    "x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)}\n",
    "$$\n",
    "\n",
    "Here, let's use the function [sklearn.preprocessing.MinMaxScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to rescale our $X$.\n",
    "\n",
    "And check the range of each column of $X$ again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(mpg_data[x_cols])\n",
    "X_scaled = np.hstack((np.ones((X_scaled.shape[0], 1)), X_scaled))    # add the column for intercept\n",
    "\n",
    "pd.DataFrame(X_scaled).describe().loc[['max', 'min']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the column \"0\" corresponds to the dummy data for intercept. All values in that column is 1.\n",
    "\n",
    "Finally, we are ready to run gradient descent to find the optimal parameters for our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iter = 1000\n",
    "alpha = 0.001\n",
    "params = np.zeros(X.shape[1])\n",
    "\n",
    "for i in range(max_iter):\n",
    "    descent = gradient(params, linear_regression, X_scaled, y)\n",
    "    params = params - descent * alpha\n",
    "    loss = mse_loss(params, linear_regression, X_scaled, y)\n",
    "    if i%100 == 0:\n",
    "        print(f\"iteration {i:3}, {loss = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the trained weights. Recall that the first element is the intercept, and the rest correspond to the 6 features respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make predictions with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gd = X_scaled @ params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that we haven't discussed till now is how to quantify the accuracy of a model. For regression problems, two basic metrics are the mean absolute error (MAE) and the root mean squared error (RMSE). The latter is just the square root of the MSE loss function that we used.\n",
    "\n",
    "$$\n",
    "\\mathrm{MAE}(\\mathbf{y}, \\hat{\\mathbf{y}})=\\frac{1}{N} \\sum_{i=1}^{N}\\left|y^{(i)}-\\hat{y}^{(i)}\\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{RMSE}(\\mathbf{y}, \\hat{\\mathbf{y}})=\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "Most common metrics are available in scikit-learn. Let's compute both errors using the corresponding functions in [`sklearn.metrics`](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(y, y_pred_gd)\n",
    "rmse = mean_squared_error(y, y_pred_gd, squared=False)\n",
    "print(f\"gradient descent\")\n",
    "print(f\"{mae  = }\")\n",
    "print(f\"{rmse = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with scikit-learn\n",
    "\n",
    "We want to mention that the `LinearRegression()` function in scikit-learn offers the same capability. Now with a thorough understanding of the model, you should feel more comfortable to use these black-boxes. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression(fit_intercept=False).fit(X, y)\n",
    "print(\"params:\")\n",
    "print()\n",
    "\n",
    "y_pred_sklearn = regressor.predict(X)\n",
    "\n",
    "mae = mean_absolute_error(y, y_pred_sklearn)\n",
    "rmse = mean_squared_error(y, y_pred_sklearn, squared=False)\n",
    "print(f\"scikit-learn linear regression\")\n",
    "print(f\"{mae  = }\")\n",
    "print(f\"{rmse = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with pseudo-inverse\n",
    "\n",
    "We want to conclude this notebook with a callback to the final lesson in the linear algebra module. Recall that we can use SVD to obtain the psuedo-inverse of a matrix and the psuedo-inverse offers a least squares solution of the corresponding linear system. Given $X$ and $\\mathbf{y}$, finding the linear regression weights $\\mathbf{w}$ that minimizes the MSE loss function is exactly a least squares problem.\n",
    "\n",
    "Performing SVDs on large dataset might not be ideal, but let's try on this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import pinv\n",
    "\n",
    "params = pinv(X) @ y\n",
    "y_pred_svd = X @ params\n",
    "\n",
    "mae = mean_absolute_error(y, y_pred_svd)\n",
    "rmse = mean_squared_error(y, y_pred_svd, squared=False)\n",
    "print(f\"linear regression using pseudo inverse\")\n",
    "print(f\"{mae  = }\")\n",
    "print(f\"{rmse = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are careful enough, you will notice that the error from using pseudo-inverse is almost identical to the error from using the scikit-learn function. In fact, that is exactly how `LinearRegression()` is implemented in scikit-learn, since a closed form solution is available. However, for more complicated models, we have to use gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
