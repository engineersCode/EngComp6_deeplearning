{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License © 2021 Lorena A. Barba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression by gradient descent\n",
    "\n",
    "This module of _Engineering Computations_ takes a step-by-step approach to introduce you to the essential ideas of deep learning, an algorithmic technology that is taking the world by storm. \n",
    "It is at the core of the artificial intelligence boom, and we think every scientist and engineer should understand the basics, at least. \n",
    "\n",
    "Another term for deep learning is deep neural networks. \n",
    "In this module, you will learn how neural-network models are built, computationally. \n",
    "The inspiration for deep learning may have been how the brain works, but in practice what we have is a method to build models, using mostly linear algebra and a little bit of calculus. \n",
    "These models are not magical, or even \"intelligent\"—they are just about _optimization_, which every engineer knows about!\n",
    "\n",
    "We start with the very basics of model-building: linear regression. \n",
    "In Module 1 of the _Engineering Computations_ series, we discuss [linear regression with real data](http://go.gwu.edu/engcomp1lesson5), and find the model parameters (slope and $y$-intercept) analytically. \n",
    "Let's forget about that for this lesson. \n",
    "The key concept here will be _gradient descent_. Start your ride here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "This lesson is partly based on a tutorial at the 2019 SciPy Conference by Eric Ma [1]. He begins his tutorial by presenting the idea of _gradient descent_ with a simple quadratic function: the question is how do we find this function's minimum?\n",
    "\n",
    "$$f(w) = w^2 +3w -5$$\n",
    "\n",
    "We know from calculus that at the minimum, the derivative of the function is zero (the tangent to the function curve is horizontal), and the second derivative is positive (the curve slants _up_ on each side of the minimum). \n",
    "The analytical derivative of the function above is $f^\\prime(w) = 2w + 3$ and the second derivative is $f^{\\prime\\prime}(w)=2>0$. Thus, we make $2w+3=0$ to find the minimum.\n",
    "\n",
    "Let's play with this function using SymPy. We'll later use NumPy, and make plots with Matplotlib, so we load all the libraries in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "import numpy\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this SymPy method to get beautiful typeset symbols and equations (in the Jupyter notebook, it will use [MathJax](https://en.wikipedia.org/wiki/MathJax) by default): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the Python variable `w` to be a SymPy symbol, and create the expression `f` to match the mathematical function above, and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = sympy.Symbol('w', real=True)\n",
    "\n",
    "f = w**2 + 3*w - 5\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.plotting.plot(f);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neat parabola. We can see from the plot that the minimum of $f(w)$ is reached somewhere betweetn $w=-2.5$ and $w=0$. SymPy can tell us the derivative, and the value where it is zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprime = f.diff(w)\n",
    "fprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.solve(fprime, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks about right: $-3/2$ or $-1.5$. The idea in gradient descent is to find the value of $w$ at the function minimum by starting with an initial guess, then iteratively taking small steps down the slope of the function, i.e., in the negative gradient direction. \n",
    "To illustrate the process, we turn the symbolic expression `fprime` into a Python function that we can call, and use it in a simple loop taking small steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpnum = sympy.lambdify(w, fprime)\n",
    "type(fpnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep. We got a Python function with the `sympy.lambdify()` method. Now, you can pick any starting guess, say $w=10$, and advance in a loop taking steps of size $0.01$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 10.0   # starting guess for the min\n",
    "\n",
    "for i in range(1000):\n",
    "    w = w - fpnum(w)*0.01 # with 0.01 the step size\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gave a result very close to the true value $-1.5$, and all we needed was a function for the derivative of $f(w)$. This is how you find the argument of the minimum of a function iteratively. \n",
    "\n",
    "##### Note\n",
    "\n",
    "> Implied in this method is that the function is differentiable, and that we can step *down* the slope, meaning its second derivative is positive, or the function is _convex_.\n",
    "\n",
    "<img src=\"../images/descent.png\" style=\"width: 400px;\"/> \n",
    "\n",
    "#### Gradient descent steps in the direction of the negative slope to approach the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Suppose you have data consisting of one independent variable and one dependent variable, and when you plot the data it seems to noisily follow a line. \n",
    "To build a model with this data, you assume the relationship is _linear_, and seek to find the line's slope and $y$-intercept (the model parameters) that best fit the data. \n",
    "\n",
    "Let's make some synthetic data to play with, following the example in Eric Ma's tutorial [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sythetic data (from Eric's example)\n",
    "x_data = numpy.linspace(-5, 5, 100)\n",
    "w_true = 2\n",
    "b_true = 20\n",
    "\n",
    "y_data = w_true*x_data + b_true + numpy.random.normal(size=len(x_data))\n",
    "\n",
    "pyplot.scatter(x_data,y_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This situation arises often. In **Module 1** of _Engineering Computations_, we used a real data set of Earth temperature over time and fit an ominously sloped line. \n",
    "We derived analytical formulas for the model coefficients and wrote our own custom functions, and we also learned that NumPy has a function that will do it for us: `numpy.polyfit(x, y, 1)` will return the two parameters $w, b$ for the line\n",
    "\n",
    "$$y = w x + b $$\n",
    "\n",
    "Here, we will use gradient descent to get the parameters of the linear model. \n",
    "The first step is to define a function that represents the _deviation_ of the data from the model. \n",
    "For linear regression, we use the sum (or the mean) of the square errors, also called _residuals_, or the difference between each data point and the predicted value from the linear model.\n",
    "\n",
    "<img src=\"../images/residuals.png\" style=\"width: 400px;\"/> \n",
    "\n",
    "#### Each data point deviates from the linear regression: we aim to minimize the sum of squares of the residuals.\n",
    "\n",
    "\n",
    "Let's review our ingredients:\n",
    "\n",
    "1. observational data, in the form of two array: $x, y$\n",
    "2. our linear model: $y = wx + b$\n",
    "3. a function that measures the discrepancy between the data and the fitting line: $\\sum (y_i - f(x_i))^2$\n",
    "\n",
    "The last item is called a \"loss function\" (also sometimes \"cost function\"). Our method will be to step down the slope of the loss function, to find its minimum.\n",
    "\n",
    "As a first approach, let's again use SymPy, which can compute derivatives for us. Below, we define the loss function for a single data point, and make Python functions with its derivative. \n",
    "We call these functions in a sequence of steps that start at an initial guess for the parameters (we choose zero), and step in the negative gradient multiplied by a step size (we choose $0.01$). \n",
    "After $1000$ steps, you see that the values of $w$ and $b$ are quite close to the true values from our synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, x, y = sympy.symbols('w b x y')\n",
    "\n",
    "loss = (w*x + b - y)**2\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_b = sympy.lambdify([w,b,x,y], loss.diff(b), 'numpy')\n",
    "grad_w = sympy.lambdify([w,b,x,y], loss.diff(w), 'numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0\n",
    "b = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    descent_b = numpy.sum(grad_b(w,b,x_data,y_data))/len(x_data)\n",
    "    descent_w = numpy.sum(grad_w(w,b,x_data,y_data))/len(x_data)\n",
    "    w = w - descent_w*0.01 # with 0.01 the step size\n",
    "    b = b - descent_b*0.01 \n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.scatter(x_data,y_data)\n",
    "pyplot.plot(x_data, w*x_data + b, '-r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Eric Ma, \"Deep Learning Fundamentals: Forward Model, Differentiable Loss Function & Optimization,\" SciPy 2019 tutorial. [video on YouTube](https://youtu.be/JPBz7-UCqRo) and [archive on GitHub](https://github.com/ericmjl/dl-workshop/releases/tag/scipy2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
