{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License © 2021 Lorena A. Barba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression by gradient descent\n",
    "\n",
    "This module of _Engineering Computations_ takes a step-by-step approach to introduce you to the essential ideas of deep learning, an algorithmic technology that is taking the world by storm. \n",
    "It is at the core of the artificial intelligence boom, and we think every scientist and engineer should understand the basics, at least. \n",
    "\n",
    "Another term for deep learning is deep neural networks. \n",
    "In this module, you will learn how neural-network models are built, computationally. \n",
    "The inspiration for deep learning may have been how the brain works, but in practice what we have is a method to build models, using mostly linear algebra and a little bit of calculus. \n",
    "These models are not magical, or even \"intelligent\"—they are just about _optimization_, which every engineer knows about!\n",
    "\n",
    "We start with the very basics of model-building: linear regression. \n",
    "In Module 1 of the _Engineering Computations_ series, we discuss [linear regression with real data](http://go.gwu.edu/engcomp1lesson5), and find the model parameters (slope and $y$-intercept) analytically. \n",
    "Let's forget about that for this lesson. \n",
    "The key concept here will be _gradient descent_. Start your ride here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "This lesson is partly based on a tutorial at the 2019 SciPy Conference by Eric Ma [1]. He begins his tutorial by presenting the idea of _gradient descent_ with a simple quadratic function: the question is how do we find this function's minimum?\n",
    "\n",
    "$$f(w) = w^2 +3w -5$$\n",
    "\n",
    "We know from calculus that at the minimum, the derivative of the function is zero (the tangent to the function is horizontal), and the second derivative is positive (the function slants _up_ on each side of the minimum). \n",
    "The analytical derivative of the function above is $f^\\prime(w) = 2w + 3$ and the second derivative is $f^{\\prime\\prime}(w)=2>0$. Thus, we make $2w+3=0$ to find the minimum.\n",
    "\n",
    "Let's play with this function using SymPy. We'll later use NumPy, and make plots with Matplotlib, so we load all the libraries in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "import numpy\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this SymPy method to get beautiful typeset symbols and equations (in the Jupyter notebook, it will use MathJax by default): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the Python variable `w` to be a SymPy symbol, and create the expression `f` to match the mathematical function above, and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = sympy.Symbol('w', real=True)\n",
    "\n",
    "f = w**2 + 3*w - 5\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.plotting.plot(f);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neat parabola. We can see from the plot that the minimum of $f(w)$ is reached somewhere betweetn $w=-2.5$ and $w=0$. SymPy can tell us the derivative, and the value where it is zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprime = f.diff(w)\n",
    "fprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.solve(fprime, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks about right: $-3/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Eric Ma, \"Deep Learning Fundamentals: Forward Model, Differentiable Loss Function & Optimization,\" SciPy 2019 tutorial. [video on YouTube](https://youtu.be/JPBz7-UCqRo) and [archive on GitHub](https://github.com/ericmjl/dl-workshop/releases/tag/scipy2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
