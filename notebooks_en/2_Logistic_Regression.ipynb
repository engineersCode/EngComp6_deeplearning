{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License Â© 2021 Lorena A. Barba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "In Lesson 1 of this module, you learned about fitting a line to data (linear regression) using the method of gradient descent to find the model parameters (slope and $y$-intercept). But what if the observational data looks nothing like it has a linear relationship?\n",
    "\n",
    "A major class of problems deals with binary classification, that is, the data belong to one of two categories. \n",
    "Typically, we code the two categories with $0$ and $1$. For example, is an email spam, or not spam? Is a credit-card transaction fraudulent, or legitimate? \n",
    "In these settings, often the data correspond to numbers between $0$ and $1$ that represents some _probability_ (e.g., that the email be spam or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic function\n",
    "\n",
    "We can build a model that will output a value between zero and one by making a non-linear transformation of the linear regression. \n",
    "This is achieved with the _logistic function_:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "With the data consisting of two arrays, $x, y$, the model is a composition: $z= wx + b$ and $\\sigma(z)$ is the output.\n",
    "\n",
    "Let's play with this function using SymPy. We'll need NumPy and Matplotlib later, so might as well load all our libraries now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "import numpy\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = sympy.Symbol('z', real=True)\n",
    "\n",
    "logistic = 1/(1+ sympy.exp(-z))\n",
    "logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.plotting.plot(logistic);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a well-groomed $S$-shaped function: it's called a _sigmoid_ curve. Notice that when $z=0$ it takes the value $0.5$, and it approaches zero on the left, and one on the right.\n",
    "\n",
    "Let's generate some synthetic data to play with. (We take this example from the SciPy 2019 tutorial by Eric Ma [1]). \n",
    "Our goal is to use gradient descent to find the model parameters $w$ and $b$ that best fit the data, in some sense that we need to discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data\n",
    "x_data = numpy.linspace(-5, 5, 100)\n",
    "w = 2\n",
    "b = 1\n",
    "z_data = w * x_data + b + numpy.random.normal(size=len(x_data))\n",
    "y_data = 1 / (1+ numpy.exp(-z_data))\n",
    "\n",
    "pyplot.scatter(x_data, y_data, alpha=0.4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic loss function\n",
    "\n",
    "To use gradient descent, we need a loss function that we can optimize with respect to the parameters. \n",
    "If we were to use a mean-square-error loss function, like in linear regression, taking the derivatives to optimize would involve $\\sigma^{\\prime}(z)$, so let's look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprime = logistic.diff(z)\n",
    "lprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.plotting.plot(lprime);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the logistic function takes very small values at the long tails on each side of $z=0$. \n",
    "If our loss function has a $\\sigma^{\\prime}(z)$ factor (coming from the chair rule), this would lead to _slow learning_. Can we work with a better loss function, that avoids this problem? (For a more detailed discussion, we recommend Chapter 3 of Michael Nielsen's free ebook [2]).\n",
    "\n",
    "We note that $\\sigma^{\\prime}(z)$ has the same expression in the denominator as $\\sigma(z)$, but squared. Let's play around with this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprime/logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we can try to express this in terms of $\\sigma(z)$: add and subtract $1$ to the numerator and factor:\n",
    "\n",
    "$$\\frac{e^{-z}}{1+e^{-z}} = \\frac{1+e^{-z}-1}{1+e^{-z}} = 1 - \\frac{1}{1+ e^{-z}} = 1 - \\sigma(z)$$\n",
    "\n",
    "We get this interesting property for the derivative of the logistic function:\n",
    "\n",
    "$$\\sigma^{\\prime}(z) = \\sigma(z) (1-\\sigma(z))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, y = sympy.symbols('a y', real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLda = (a-y)/a/(1-a)\n",
    "dLda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = sympy.integrate(dLda, a)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.simplify(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Eric Ma, \"Deep Learning Fundamentals: Forward Model, Differentiable Loss Function & Optimization,\" SciPy 2019 tutorial. [video on YouTube](https://youtu.be/JPBz7-UCqRo) and [archive on GitHub](https://github.com/ericmjl/dl-workshop/releases/tag/scipy2019).\n",
    "2. Michael A. Nielsen, \"Neural Networks and Deep Learning\" (2015), Determination Press, http://neuralnetworksanddeeplearning.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
