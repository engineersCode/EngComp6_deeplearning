{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License © 2021 Lorena A. Barba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "In Lesson 1 of this module, you learned about fitting a line to data (linear regression) using the method of gradient descent to find the model parameters (slope and $y$-intercept). But what if the observational data looks nothing like it has a linear relationship?\n",
    "\n",
    "A major class of problems deals with binary classification, that is, the data belong to one of two categories. \n",
    "Typically, we code the two categories with $0$ and $1$. For example, is an email spam, or not spam? Is a credit-card transaction fraudulent, or legitimate? \n",
    "In these settings, often the data correspond to numbers between $0$ and $1$ that represent some _probability_ (e.g., that the email be spam or not).\n",
    "One effective way to build a model representing this situation is logistic regression. The output of the model should be a number between $0$ and $1$, for inputs in the whole real line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic function\n",
    "\n",
    "We can build a model that will output a value between zero and one by making a non-linear transformation of the linear regression. \n",
    "This is achieved with the _logistic function_:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "With the data consisting of two arrays, $x, y$, the model is a composition: $z= wx + b$ and $\\sigma(z)$ is the output.\n",
    "The task is finding $w$ and $b$ so that $\\sigma(z)$ is closest to $y$ for all data points.\n",
    "\n",
    "Let's play with this function using SymPy. We'll need NumPy and Matplotlib later, so might as well load all our libraries now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "import numpy\n",
    "\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = sympy.Symbol('z', real=True)\n",
    "\n",
    "logistic = 1/(1+ sympy.exp(-z))\n",
    "logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.plotting.plot(logistic);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a well-groomed $S$-shaped function: it's called a _sigmoid_ curve. Notice that when $z=0$ it takes the value $0.5$, and it approaches zero on the left, and one on the right.\n",
    "\n",
    "Let's generate some synthetic data to play with. (We take this example from the SciPy 2019 tutorial by Eric Ma [1]). \n",
    "Our goal is to use gradient descent to find the model parameters $w$ and $b$ that best fit the data, in some sense that we need to discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data\n",
    "x_data = numpy.linspace(-5, 5, 100)\n",
    "w = 2\n",
    "b = 1\n",
    "z_data = w * x_data + b + numpy.random.normal(size=len(x_data))\n",
    "y_data = 1 / (1+ numpy.exp(-z_data))\n",
    "\n",
    "pyplot.scatter(x_data, y_data, alpha=0.4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic loss function\n",
    "\n",
    "To use gradient descent, we need a loss function that we can optimize with respect to the parameters. \n",
    "If we were to use a mean-square-error loss function, like in linear regression, we run into a few problems. Let's discuss.\n",
    "\n",
    "Taking the derivatives to optimize would involve $\\sigma^{\\prime}(z)$, so let's look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprime = logistic.diff(z)\n",
    "lprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.plotting.plot(lprime);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the logistic function takes very small values at the long tails on each side of $z=0$. \n",
    "If our loss function has a $\\sigma^{\\prime}(z)$ factor (coming from the chain rule), this would lead to _slow learning_. \n",
    "We'd like to work with a better loss function, that avoids this problem, and we build one below by integration. (For a more detailed discussion, we recommend Chapter 3 of Michael Nielsen's free ebook [2]).\n",
    "\n",
    "It's important to note also that our prediction model is a nonlinear function, composed with the linear model, and the square-error would lead to a non-convex loss function that can have local minima, and make gradient descent fail. \n",
    "Here's an example posted on Stackoverflow in answer to this very [question](https://math.stackexchange.com/questions/2381724/logistic-regression-when-can-the-cost-function-be-non-convex). Consider just three data points, and a model with no intercept, $z = wx$: $(-1, 2), (-20, -1), (-5, 5)$. What would a square-error mean function look like? We can plot it using SymPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badloss = (1/(1+ sympy.exp(-z))-2)**2 + \\\n",
    "          (1/(1+ sympy.exp(-20*z))+1)**2  + \\\n",
    "          (1/(1+ sympy.exp(-5*z))-5)**2\n",
    "badloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.plotting.plot(badloss, xlim=(-1,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a local minimum, where gradient descent could get stuck!\n",
    "\n",
    "But let's go back to inspecting the derivative of the logistic function. \n",
    "We note that $\\sigma^{\\prime}(z)$ has the same expression in the denominator as $\\sigma(z)$, but squared. Let's play around with this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lprime/logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we can try to express this in terms of $\\sigma(z)$: add and subtract $1$ to the numerator and factor:\n",
    "\n",
    "$$\\frac{e^{-z}}{1+e^{-z}} = \\frac{1+e^{-z}-1}{1+e^{-z}} = 1 - \\frac{1}{1+ e^{-z}} = 1 - \\sigma(z)$$\n",
    "\n",
    "We get this interesting property for the derivative of the logistic function:\n",
    "\n",
    "$$\\sigma^{\\prime}(z) = \\sigma(z) (1-\\sigma(z))$$\n",
    "\n",
    "Following Nielsen [2], let's call $\\sigma(z)=a$, representing the predicted value by the model where $z=wx +b$. \n",
    "The deviation between the model output and the observational data at each point is measured by $|y-a|$. \n",
    "A square-error loss function, as used for linear regression, is $(y-a)^2$, giving derivatives with respect to the parameters that are proportional to the error: \n",
    "the larger the error, the larger the step taken in gradient descent. \n",
    "That's a nice property.\n",
    "\n",
    "Seeking a loss function $L$ with the same behavior, we would like, for example:\n",
    "$\\frac{\\partial L}{\\partial b} = (y-a)$. \n",
    "Using the chain rule,\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial a}\\frac{\\partial a}{\\partial z} = \\frac{\\partial L}{\\partial a} \\sigma^{\\prime}(z) = \\frac{\\partial L}{\\partial a} \\, a(1-a)$$\n",
    "\n",
    "using the property of $\\sigma^{\\prime}$ in the last step.\n",
    "\n",
    "Our desired behavior for $L$ is that this derivative be $(y-a)$, giving:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial a} = \\frac{(y-a)}{a(1-a)}$$\n",
    "\n",
    "\n",
    "This lets us construct $L$ by integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, y = sympy.symbols('a y', real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLda = (y-a)/a/(1-a)\n",
    "dLda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = sympy.integrate(dLda, a)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.simplify(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Plus a constant of integration.) We note that since $a<1$ (and the logarithm is only defined for positive real numbers) we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = -y*sympy.log(a) + (y-1)*sympy.log(1-a)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.simplify(L.diff(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative is the same expression we started with. \n",
    "This is the _logistic loss function_ for a single data point:\n",
    "\n",
    "$$L = -y \\log(a) + (y-1) \\log(1-a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect this loss function. If the true value is $y=1$, we get $L= -\\log(a)$. \n",
    "Plotting in the range of interest shows that the gradient is largest when the model output is furthest away from the true value. \n",
    "This is a nice property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.plotting.plot(-sympy.log(a), xlim=(0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $y=0$: $L= - \\log(1-a)$. The plot again shows that the gradient is largest when the model output is furthest from the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.plotting.plot(-sympy.log(1-a), xlim=(0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the parameters using `autograd`\n",
    "\n",
    "With the logistic loss function, we're ready to use gradient descent in an optimization loop. \n",
    "Above, we defined the `logistic` SymPy expression as a function of the symbol `z`, so let's re-define it to include the composition with $wx+b$. \n",
    "We could then define the loss function symbolically, as we did for linear regression; have a look at this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, x, y = sympy.symbols('w b x y')\n",
    "logistic = 1/(1+ sympy.exp(-w*x-b)) # redefined with the composition\n",
    "\n",
    "Loss = -y*sympy.log(logistic) - (1-y)*sympy.log(1-logistic)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can imagine that it will be rather messy to use SymPy for obtaining the derivatives of this loss function symbolically, then transforming them into Python functions, and then writing a gradient descent loop. \n",
    "Have a look at the derivative with respect to the parameter $b$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss.diff(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a better way! It's called _automatic differentiation_: the idea is to algorithmically obtain derivatives of numeric functions written in computer code. \n",
    "It sounds like magic, but it can be done by a combination of the chain rule, symbolic rules of differentiation for elementary operations, and a numeric evaluation trace of the elementary derivatives. \n",
    "\n",
    "The details of how automatic differentiation works are fascinating, but here we skip ahead to say that software libraries are freely available to do this! \n",
    "For Python programs, the [`autograd`](https://github.com/hips/autograd) package was specifically developed for gradient-based optimization. \n",
    "It is described thus:\n",
    "\n",
    "> Autograd can automatically differentiate native Python and NumPy code. It can handle a large subset of Python's features, including loops, ifs, recursion and closures, and it can even take derivatives of derivatives of derivatives.\n",
    "\n",
    "(The package is no longer in active development, as a new version was folded into the [JAX](https://github.com/google/jax) project, out of Google. It is still maintained, though, and just as useful as ever!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the package, you can run the following command (either on the terminal or in a Jupyter code cell):\n",
    "\n",
    "```\n",
    "pip install autograd\n",
    "```\n",
    "\n",
    "or, if you installed your Python environment via Anaconda, you can run this command:\n",
    "```\n",
    "conda install -c conda-forge autograd\n",
    "```\n",
    "\n",
    "Once the package is installed in your working environment, you can import it in the usual Pythonic way. \n",
    "The main function you will need is `autograd.grad()`, which takes a scalar-valued Python function as argument, and returns another function that evaluates to its derivative. \n",
    "It's what we use in the optimization loop to perform gradient descent.\n",
    "\n",
    "In addition, `autograd.numpy` is a wrapper to the NumPy library. This allows you to call your favorite NumPy methods with `autograd` keeping track of every operation so it can give you the derivative (via the chain rule).\n",
    "We ill import it using the alias (`as np`), consistent with the tutorials and documentation that you will find online.\n",
    "Up to now in the _Engineering Computations_ series of modules, we have refrained from using the aliased form of the import statements, just to have more explicit and readable code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the autograd-wrapped version of numpy\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the gradient calculator\n",
    "from autograd import grad  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to realize that we now need to design our program in a _functional_ style: defining Python functions for the logistic and loss functions, so that we may apply `autograd.grad()` in the optimization loop. \n",
    "Study the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: the namespace np is the autograd wrapper to NumPy\n",
    "\n",
    "def logistic(z):\n",
    "    '''The logistic function'''\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "def logistic_model(params, x):\n",
    "    '''A prediction model based on the logistic function composed with wx+b\n",
    "       params: array(w,b) of model parameters\n",
    "       x :  array of x data'''\n",
    "    w = params[0]\n",
    "    b = params[1]\n",
    "    z = w * x + b\n",
    "    y = logistic(z)\n",
    "    return y\n",
    "\n",
    "def log_loss(params, model, x, y):\n",
    "    '''The logistic loss function'''\n",
    "    y_pred = model(params, x)\n",
    "    return -np.mean(y * np.log(y_pred) + (1-y) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a function to compute the gradient of the logistic loss\n",
    "gradient = grad(log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "\n",
    "> The argument list of the function returned by `grad()` will be the same as the argument list of the loss function that we input to it.\n",
    "\n",
    "Let's now make a random starting guess for the two model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.random.rand(2)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With values assigned to the two parameters, this is how you compute the corresponding derivatives—we'll need to do this in each iteration of the optimization loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(params, logistic_model, x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we optimize! Notice that we set both a maximum number of iterations, and an exit criterion based on two successive residuals being very close. \n",
    "\n",
    "Finally, we plot the synthetic data we created above, and the logistic regression curve corresponding to the parameters found in the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3000):\n",
    "    descent = gradient(params, logistic_model, x_data, y_data)\n",
    "    oldparams = params\n",
    "    params = params - descent * 0.01\n",
    "    residual = np.abs((params - oldparams) / oldparams)\n",
    "    if np.all(residual < 1e-6):\n",
    "        break\n",
    "\n",
    "print(f'Optimized value of w is {params[0]:.3f} vs. true value: 2')\n",
    "print(f'Optimized value of b is {params[1]:.3f} vs. true value: 1')\n",
    "print(f'Exited after {i} iterations')\n",
    "print(f'Residual is {residual}')\n",
    "\n",
    "pyplot.scatter(x_data, y_data, alpha=0.4)\n",
    "pyplot.plot(x_data, logistic_model(params, x_data), '-r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! We found a logistic curve that approximates the data quite well. \n",
    "This curve represents the _prediction model_ for the continuous data between $0$ and $1$. \n",
    "In a classification task, the next step is to assign a decision threshold for the two classes; a natural choice is $0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(y):\n",
    "    return 1 if y >= .5 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will code as $0$ or $1$ each data point. We will want to apply the function element-wise to all elements in an array, for which we can use [`numpy.vectorize()`](https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html). \n",
    "Be sure to read the documentation of this very useful NumPy function that saves you from having to write a for-loop!\n",
    "\n",
    "The decision rule is what turns logistic regression into a classification tool. Check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary = numpy.vectorize(decision_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(predictions):\n",
    "    '''\n",
    "    Argument:\n",
    "    predictions, an array of values between 0 and 1\n",
    "    \n",
    "    Returns: \n",
    "    classified, an array of 0 and 1 values'''\n",
    "\n",
    "    return decision_boundary(predictions).flatten()\n",
    "\n",
    "pyplot.scatter(x_data, classify(x_data), alpha=0.4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the end of Lesson 2 of our module. We spent some time deriving the logistic loss function, where most sources will just give it to you without explanation. \n",
    "We find it more satisfying this way, but you can certainly skip it. \n",
    "\n",
    "The key ingredients we have added in this lesson are the ideas of:\n",
    "- using composition of functions in our model\n",
    "- using automatic differentiation to get the gradients for the optimization loop\n",
    "\n",
    "In the next lesson, we add the idea of multiple independent variables (a.k.a. _features_) affecting our model, leading to **multiple linear regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Eric Ma, \"Deep Learning Fundamentals: Forward Model, Differentiable Loss Function & Optimization,\" SciPy 2019 tutorial. [video on YouTube](https://youtu.be/JPBz7-UCqRo) and [archive on GitHub](https://github.com/ericmjl/dl-workshop/releases/tag/scipy2019).\n",
    "2. Michael A. Nielsen, \"Neural Networks and Deep Learning\" (2015), Determination Press, http://neuralnetworksanddeeplearning.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
