{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License Â© 2021 Lorena A. Barba, Tingyu Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "In this fourth lesson of the _Engineering Computations_ module on deep learning, we play with a different model-building method: polynomial regression. \n",
    "We already saw one case in which the observational data could not be fit by a line, and that was the classification problem that we tackled with logistic regression (Lesson 3). \n",
    "Now imagine that your data wiggles about on the $x, y$ planeâ€”where $x$ is the independent variable or feature, and $y$ is the dependent variable. \n",
    "You look at it and think: a curvilinear relationship might work. Good idea. \n",
    "\n",
    "It may surprise you to learn that fitting a polynomial to the data is a special case of multiple linear regression. Yes: _linear_. \n",
    "When we talk about linear models, we mean linear with respect to the parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A special case of multiple linear regression\n",
    "\n",
    "Let's generate some synthetic data using a polynomial function of fourth order, $y = x^4 + x^3 - 4x^2 $, with a bit of added noise.\n",
    "\n",
    "As usuall, we start by loading some needed Python libraries and functions, including the useful tools from `autograd` that you've learned about already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from autograd import grad\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)    # fix seed for reproducibility\n",
    "x = np.linspace(-3, 3, 20)\n",
    "y = x**4 + x**3 - 4*x**2 + 8*np.random.normal(size=len(x))\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we were only given the data (not the function that generated them), and our goal is to fit a curve to these data. The nonlinear relationship between $x$ and $y$ suggests that a linear regression will fail. Intuitively, using polynomial functions may first come to your mind.\n",
    "\n",
    "Let's write the model as a $d$th-order polynomial on $x$, the only feature:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\cdots + w_d x^d, \n",
    "$$\n",
    "\n",
    "where $w$ denotes the weights. Keep in mind that in the model-fitting context, the objective is always to find the optimal values of these weights given $x$ and $y$. When viewed from a different perspective, the model above is just a linear combination of the weights. In fact, by creating polynomial features of $x$, namely, letting $x_i = x^i$, the model becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_d x_d.\n",
    "$$\n",
    "\n",
    "As you can see, the polynomial regression model is identical to multiple linear regression (MLR), with the matrix form being also $\\hat{\\mathbf{y}} = X\\mathbf{w}$, and the only gap is to form the matrix $X$ using the powers of $x$.\n",
    "\n",
    "Suppose we want to fit our data with a 3rd degree polynomial function; let's write a function to create these polynomial features first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "\n",
    "def polynomial_features(x, degree):\n",
    "    \"\"\" Generate polynomial features for x.\"\"\"\n",
    "    \n",
    "    X = np.empty((len(x), degree+1))\n",
    "    for i in range(degree+1):\n",
    "        X[:,i] = x**i\n",
    "    return X\n",
    "\n",
    "X = polynomial_features(x, degree)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "\n",
    "> Unsurprisingly, **scikit-learn** offers a counterpart: [`PolynomialFeatures()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "When your original data come with multiple features (e.g., $x_1$ and $x_2$), the polynomial features for regression will involve interaction terms (e.g., $x_1 x_2$ for 2nd-order polynomials).\n",
    "In that case, it is handy to use this function to generate all terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data, train the model\n",
    "\n",
    "Next, recall that for MLR, we should normalize each feature to the same scale.\n",
    "As in the previous lesson, let's use `MinMaxScaler()` to scale all features to $[0,1]$, except for the first column:\n",
    "$x_0$ is set to 1 for all entries, since $w_0$ represents the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "X_scaled[:,0] = 1   # the column for intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scaling the matrix of polynomial features $X$ above, we used:\n",
    "```python\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "```\n",
    "The function `fit_transform()` is actually a combination of two steps:\n",
    "- [`fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.fit): compute $x_{\\min}$ and $x_{\\max}$ values for each feature and save the information to the variable `min_max_scaler`. You can access them via `min_max_scaler.data_min_` and `min_max_scaler.data_max_`.\n",
    "- [`transform()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.transform) use `min_max_scaler` to scale `X`.\n",
    "\n",
    "This will be helpful to remember a bit later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse the same model and loss function from Lesson 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(params, X):\n",
    "    '''\n",
    "    The linear regression model in matrix form.\n",
    "    Arguments:\n",
    "      params: 1D array of weights for the linear model\n",
    "      X     : 2D array of input values\n",
    "    Returns:\n",
    "      1D array of predicted values\n",
    "    '''\n",
    "    return np.dot(X, params)\n",
    "\n",
    "def mse_loss(params, model, X, y):\n",
    "    '''\n",
    "    The mean squared error loss function.\n",
    "    Arguments:\n",
    "      params: 1D array of weights for the linear model\n",
    "      model : function for the linear regression model\n",
    "      X     : 2D array of input values\n",
    "      y     : 1D array of predicted values\n",
    "    Returns:\n",
    "      float, mean squared error\n",
    "    '''\n",
    "    y_pred = model(params, X)\n",
    "    return np.mean( np.sum((y-y_pred)**2) )\n",
    "\n",
    "gradient = grad(mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, \"training\" a model simply means finding the best parameters by minimizing a loss function. \n",
    "We'll choose both a maximmum number of iterations in the optimization loop, and an exit criterion based on the norm of the gradient at the current iteration. \n",
    "(If this is quite small, when multiplied by the also small learning rate, the parameters will change very little.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 3000\n",
    "alpha = 0.01\n",
    "params = np.zeros(X_scaled.shape[1])\n",
    "descent = np.ones(X_scaled.shape[1])\n",
    "i = 0\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "while np.linalg.norm(descent) > 0.01 and i < max_iter:\n",
    "    descent = gradient(params, linear_regression, X_scaled, y)\n",
    "    params = params - descent * alpha\n",
    "    loss = mse_loss(params, linear_regression, X_scaled, y)\n",
    "    mae = mean_absolute_error(y, X_scaled@params)\n",
    "    if i%100 == 0:\n",
    "        print(f\"iteration {i:4}, {loss = :.3f}, {mae = :.3f}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is the intercept, and the rest are the weights of each **scaled** polynomial feature.\n",
    "\n",
    "Although our model has multiple weights, there's only one feature in our original data. \n",
    "Therefore, if we are given new values of $x$ to predict $y$, we need to first create the polynomial features, scale them to $[0,1]$ using the **same scaling function**, and then multiply by the weights. Since we have used the min-max scaling, it is important to use the same $x_{\\min}$ and $x_{\\max}$ of the training data to scale new data.\n",
    "\n",
    "Recall the explanation above for the scaling step. We now only need to call `min_max_scaler.transform()` to scale new data. Such design occurs very often in scikit-learn.\n",
    "\n",
    "With that in mind, let's plot the fitted curve together with the data. We generate some new values of $x$, `xgrid`, and predict $y$ at these locations. Don't forget to repeat the procedure of creating polynomial features and scaling them. Ponder over this code for a moment to wrap your head around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrid = np.linspace(x.min(), x.max(), 30)\n",
    "Xgrid_poly_feat = polynomial_features(xgrid, degree)\n",
    "Xgrid_scaled = min_max_scaler.transform(Xgrid_poly_feat)\n",
    "Xgrid_scaled[:,0] = 1 \n",
    "plt.scatter(x, y, c='r', label='true')\n",
    "plt.plot(xgrid, Xgrid_scaled@params, label='predicted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe underfitting & overfitting\n",
    "\n",
    "In the model above, we just randomly picked a polynomial degree of $3$ for the fitted curve. Is it good enough to model our dataset? Should we try higher-order polynomials?\n",
    "\n",
    "We can repeat our study with different polynomial degrees varying from $1$ to $15$, and see what happens.\n",
    "To faciliate this task, we provide you with a script to train the model and plot these fitted curves interactively using `ipywidget`.\n",
    "\n",
    "Run the cell below and drag the slider to see how the curve changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "from plot_helpers import interact_polyreg\n",
    "\n",
    "max_degree = 15\n",
    "interact_polyreg(max_degree, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "\n",
    "When `degree` is $1$, the straight line clearly fails to capture the underlying relationship between $x$ and $y$. Specifically, a line is too simple to explain how far the data are spread out, namely, the **variance** in the data. We say that the linear model **underfits** the data in this case.\n",
    "\n",
    "Underfitting happens when the model is too naive or the weights need to be trained with more iterations. It is often easy to detect, since a large training error is a good indicator.\n",
    "\n",
    "##### Challenge question\n",
    "\n",
    "> Would having more training data help resolve underfitting?\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "As we increase the polynomial degree, the training error (MAE from the figure title) keeps decreasing. \n",
    "But does it mean that the 15th-order polynomial gives the best fit?\n",
    "Probably not.\n",
    "\n",
    "Drag the slider to the right, and you will find that the curve passes exactly through many points and looks very odd.\n",
    "If we are given new data, this model may not predict well because it fits too closely to the old data.\n",
    "As real-world data tend to be noisy (due to missing and erroneous values), our synthetic data also have noise added.\n",
    "Models with high polynomial degrees are so flexible that they fit the noise rather than the true relationship! \n",
    "In this case, these models are **overfitting**, or have a **high variance**.\n",
    "Overfitting usually happens when the model is overly complicated.\n",
    "The high-order polynomials in our example have too many degrees of freedom (weights) for our data.\n",
    "\n",
    "##### Challenge question\n",
    "\n",
    "> Would having more training data help resolve overfitting?\n",
    "\n",
    "Compared to underfitting, overfitting is in general harder to detect because the training error tends to be small. We will discuss how to identify overfitting later in this module.\n",
    "Now let's focus on how to prevent overfitting using regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is used to reduce or avoid overfitting.\n",
    "The idea is straighforward: introduce a term in the loss function that penalizes complicated models. In our polynomial regression model:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\cdots + w_d x^d,\n",
    "$$\n",
    "\n",
    "every polynomial term contributes to the overall complexity of the model.\n",
    "A simple idea is to add constraints to the magnitudes of these weights.\n",
    "\n",
    "By adding a regularization term $\\sum_{j=1}^d w_j^2$ to the MSE loss $\\frac{1}{N} {\\lVert \\mathbf{y} - X\\mathbf{w} \\rVert}^2$, the new loss\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w})=\\frac{1}{N} {\\lVert \\mathbf{y} - X\\mathbf{w} \\rVert}^2 + \\lambda \\sum_{j=1}^d w_j^2\n",
    "$$\n",
    "\n",
    "now favors smaller weights, because larger weights will increase the total loss.\n",
    "As weights are smaller (closer to zero), the model tends to be simpler, thus is less likely to overfit. Notice that we don't penalize the intercept $w_0$ in the regularization term, there are mathematical explanations for this, but one way to think about it is that a constant term won't affect the overall model complexity.\n",
    "\n",
    "Above, $\\lambda$ denotes the regularization parameter, or the strength of penalty. It controls the tradeoff between fitting the data well (reducing the first term) and keeping the model simple to avoid overfitting (reducing the second term). When $\\lambda$ is tiny, the loss falls back to regular MSE loss; when $\\lambda$ is huge, all penalized weights will be almost 0 after training and our model would just be a constant value $w_0$.\n",
    "\n",
    "Let's code the regularized MSE loss and set $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_loss(params, model, X, y, _lambda=1.0):\n",
    "        '''\n",
    "    The mean squared error loss function with an L2 penalty.\n",
    "    Arguments:\n",
    "      params: 1D array of weights for the linear model\n",
    "      model : function for the linear regression model\n",
    "      X     : 2D array of input values\n",
    "      y     : 1D array of predicted values\n",
    "      _lambda: regularization parameter, default 1.0\n",
    "    Returns:\n",
    "      float, regularized mean squared error\n",
    "    '''\n",
    "    y_pred = model(params, X)\n",
    "    return np.mean( np.sum((y-y_pred)**2) ) + _lambda * np.sum( params[1:]**2 )\n",
    "\n",
    "gradient = grad(regularized_loss)                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_regularization_params = params.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train the 3rd-degree polynomial model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 3000\n",
    "alpha = 0.01\n",
    "params = np.zeros(X_scaled.shape[1])\n",
    "descent = np.ones(X_scaled.shape[1])\n",
    "i = 0\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "while np.linalg.norm(descent) > 0.01 and i < max_iter:\n",
    "    descent = gradient(params, linear_regression, X_scaled, y)\n",
    "    params = params - descent * alpha\n",
    "    loss = mse_loss(params, linear_regression, X_scaled, y)\n",
    "    mae = mean_absolute_error(y, X_scaled@params)\n",
    "    if i%100 == 0:\n",
    "        print(f\"iteration {i:4}, {loss = :.3f}, {mae = }\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the optimal weights before and after regularization. We can reuse the `xgrid` to plot both curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"weights without regularization\")\n",
    "print(no_regularization_params)\n",
    "print(\"weights with regularization\")\n",
    "print(params)\n",
    "\n",
    "plt.scatter(x, y, c='r', label='true')\n",
    "plt.plot(xgrid, Xgrid_scaled@no_regularization_params, label='w/o regularization')\n",
    "plt.plot(xgrid, Xgrid_scaled@params, label='with regularization')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our helper script again to display both models with varying polynomial degree in an `ipywidget`, interact with the slider to explore the results. We set `regularized=True` this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_polyreg(max_degree, x, y, regularized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the regularization term, you won't see the wiggling curves even with a high degree polynomial, since the magnitude of the weights (the coefficients before each polynomial term) is much smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
