{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From multiple logistic regression to the very first deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lesson 5, we showed how to apply a logistic regression model to identify defective metal-casting parts. However, the accuracy seems not good enough. This lesson will improve the accuracy by replacing the logistic model with a more advanced model: a fully connected neural network model with one hidden layer. Fully connected neural networks are the cornerstone of deep learning and so-called artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import numpy\n",
    "from autograd import grad\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we will use random numbers in this lesson. As this is a teaching material, we want to control the generation of random numbers to make it NOT really random -- you will get the same results as we did when executing this notebook by fixing the random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, our example application is identifying defective metal-casting parts using computer vision. We already saw this application in lesson 5. To save space, we wrapped the code loading and data normalizing in file `scripts/load_casting_data.py`. Execute the following code to get the data from the wrapper: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../scripts\")\n",
    "from load_casting_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code cell loads the following variables: `res`, `n_ok_total`, `n_ok_train`, `n_ok_val`, `n_ok_test`, `n_def_total`, `n_def_train`, `n_def_val`, `n_def_test`, `images_train`, `images_val`, `images_test`, `labels_train`, `labels_val`, `labels_test`, `mu`, and `sigma`. Though the variable names should be self-explanatory enough, it's still a good idea to revisit lesson 5 for the meaning of these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we will reuse some functions from lesson 5. We wrapped these functions in `scripts/lesson_5_functions.py`. Let's also import these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lesson_5_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will import `logistic`, `classify`, and `performance`. They are the same code as what we used in lesson 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to fully connected neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are fully connected neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While neural networks in machine learning may be inspired by real neural networks in our bodies, from an engineering perspective, they are just mathematical models and nothing magical. Most of them can be expressed with high-school math. Recall that a mathematical model is a hypothesized relationship between input features $\\mathbf{x}$ and outputs $\\mathbf{y}$. For example, a linear regression model assumes $\\mathbf{y} \\approx \\hat{\\mathbf{y}} = W^\\mathsf{T}\\mathbf{x}+\\mathbf{b}$, and a logistic regression model assumes $\\mathbf{y}\\approx\\hat{\\mathbf{y}}=\\operatorname{logistic}\\left(W^\\mathsf{T}\\mathbf{x}+\\mathbf{b}\\right)$. Neural networks assume a more complicated relationship between $\\mathbf{x}$ and $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no unanimous definition of neural networks. However, fully connected neural networks are commonly deemed the most basic type of neural network models among various kinds of neural networks. An input vector $\\mathbf{x}$ goes through several consecutive and interleaved linear and nonlinear transformations in fully connected neural networks before the model returns $\\hat{\\mathbf{y}}$. A linear transformation is something like $W^\\mathsf{T}\\mathbf{x}$, and a nonlinear transformation may be a logistic function, a trigonometric function, or any functions that are not linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a single input vector $\\mathbf{x}$, a fully connected neural network model returns $\\hat{\\mathbf{y}}$ through: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}^1 &= \\sigma_0\\left(\\left(W^0\\right)^\\mathsf{T}\\mathbf{x} + \\mathbf{b}^0\\right) \\\\\n",
    "\\mathbf{z}^2 &= \\sigma_1\\left(\\left(W^1\\right)^\\mathsf{T}\\mathbf{z}^1 + \\mathbf{b}^1\\right) \\\\\n",
    "&\\vdots \\\\\n",
    "\\hat{\\mathbf{y}} &= \\sigma_L\\left(\\left(W^L\\right)^\\mathsf{T}\\mathbf{z}^L + \\mathbf{b}^L\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Or, if we prefer an iterative expression:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}^0 &\\equiv \\mathbf{x} \\\\\n",
    "\\mathbf{z}^{i+1} &= \\sigma_{i}\\left(\\left(W^i\\right)^\\mathsf{T}\\mathbf{z}^{i} + \\mathbf{b}^{i}\\right)\\text{ for }i=0,\\dots,L \\\\\n",
    "\\hat{\\mathbf{y}} &\\equiv \\mathbf{z}^{L+1}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use bold-face symbols because these variables are vectors and matrices, as seen in lesson 6. The math equations of the linear and logistic regression models in lessons 1 to 5 are special cases of the vector-matrix form. Let's take a look at these symbols:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear transformations and model parameters: $W^0$, $\\mathbf{b}^0$, $\\dots$, $W^L$, $\\mathbf{b}^L$\n",
    "  \n",
    "  $W^0$, $\\mathbf{b}^0$, $\\dots$, $W^L$, $\\mathbf{b}^L$ are parameter matrices and vectors. They are responsible for linear transformations, just like in previous lessons. The difference in this lesson is that now we have more than one linear transformation. We will determine the values of these parameters using gradient descent in this lesson.\n",
    "  \n",
    "  Note that the superscript of the parameter matrices and vectors starts from $0$. Usually, in math expression, we start indices with $1$. For example, the first element in a vector $\\mathbf{x}$ is $x_1$. However, we begin with $0$ for $W$ and $\\mathbf{b}$ because we want to address how many ***additional*** transformations we have in the model besides the last transformation, i.e., the one that returns $\\mathbf{y}$. This information is carried by the variable $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nonlinear transformations:  $\\sigma_0$, $\\sigma_1$, $\\dots$, $\\sigma_{L}$\n",
    "    \n",
    "  $\\sigma_0$, $\\sigma_1$, $\\dots$, $\\sigma_{L}$ are choosen nonlinear functions. It's not uncommon to use the same function for all of them, i.e., $\\sigma_0 = \\sigma_1 = \\dots = \\sigma_{L} = \\sigma$. In addition, they are often element-wise functions. That says, if we give a vector $\\mathbf{g}$ to $\\sigma_0$, then\n",
    "  \n",
    "  $$\n",
    "  \\sigma_0\\left(\\mathbf{g}\\right)=\n",
    "  \\sigma_0\\left(\\begin{bmatrix}g_1 \\\\ g_2 \\\\ \\vdots \\end{bmatrix} \\right)=\n",
    "  \\begin{bmatrix}\\sigma_0(g_1) \\\\ \\sigma_0(g_2) \\\\ \\vdots \\end{bmatrix}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intermediate results: $\\mathbf{z}^1$, $\\mathbf{z}^2$, $\\dots$, $\\mathbf{z}^L$\n",
    "  \n",
    "  $\\mathbf{z}^1$, $\\mathbf{z}^2$, $\\dots$, $\\mathbf{z}^L$ are intermediate results. In rare situations, we can explain the meaning of these intermediate results. For example, if we are modeling a natural phenomenon, these results may represent some physical mechanisms or properties. However, in most applications, these intermediate results are just the side products of the calculations and do not carry any meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of linear-nonlinear-transformation pairs: $L$\n",
    "  \n",
    "  $L$ is a user-defined parameter that controls how many linear-nonlinear transformation pairs before the model returns the output $\\mathbf{y}$. We use $L$ to adjust the complexity of a fully connected neural network model: larger $L$ makes our model more complicated. And a more complicated neural network can model a more sophisticated relationship between $\\mathbf{x}$ and $\\mathbf{y}$.\n",
    "  \n",
    "  As mentioned previously, $L$ denotes how many additional transformations we have in the model before the one returning the final result. Alternatively, we can also treat $L$ as how many intermediate results we have in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lengths of intermediate vectors: $n_1$, $n_2$, $\\dots$, $n_L$\n",
    "  \n",
    "  $n_1$, $n_2$, $\\dots$, $n_L$ are not visible in the above matrix-vector form of the model. They represent the length of the intermediate vectors $\\mathbf{z}^1$, $\\mathbf{z}^2$, $\\dots$, $\\mathbf{z}^L$. These variables are user-defined and also control the complexity of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose $\\sigma_0(\\mathbf{g}) = \\sigma_1(\\mathbf{g}) = \\dots = \\sigma_{L}(\\mathbf{g}) = \\operatorname{logistic}(\\mathbf{g}) = \\frac{1}{1+\\mathrm{e}^{-\\mathbf{g}}}$, then a fully connected neural network is a chain of several logistic regression models. In fact, ***a logistic regression model is a fully connected neural network*** with $L=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following configuration for the fully-connected neural network in this lesson:\n",
    "\n",
    "1. $L=1$,\n",
    "2. $\\sigma_0(\\mathbf{g})=\\sigma_1(\\mathbf{g})=\\operatorname{logistic}(\\mathbf{g})$, and\n",
    "3. $n_1=64$, meaning $\\mathbf{z}^1=\\begin{bmatrix}z_1^1 & z_2^1 & \\cdots & z_{64}^1 \\end{bmatrix}^\\mathsf{T}$\n",
    "\n",
    "Also, each of our images has a total of $128\\times 128=16384$ pixels (revisit lesson 5 if you don't remember), so $\\mathbf{x}=\\begin{bmatrix}x_1 & x_2 & \\cdots & x_{16384}\\end{bmatrix}^\\mathsf{T}$.\n",
    "\n",
    "If you are not comfortable using math symbols for linear algebra, it's always nice to expand the symbols with real vectors and matrices. Let's do it with the model we'll use later in this lesson:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  z_1^1 \\\\ \\vdots \\\\ z_{64}^1\n",
    "  \\end{smallmatrix}\\right]\n",
    "  &=\n",
    "  \\operatorname{logistic}\n",
    "  \\left(\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  W_{1,1}^0 & \\cdots & W_{1,64}^0 \\\\\n",
    "  \\vdots    & \\ddots & \\vdots \\\\\n",
    "  W_{16384,1}^0 & \\cdots & W_{16384,64}^0 \\\\\n",
    "  \\end{smallmatrix}\\right]^\\mathsf{T}\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  x_1 \\\\ \\vdots \\\\ x_{16384}\n",
    "  \\end{smallmatrix}\\right]\n",
    "  +\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  b_1^0 \\\\ \\vdots \\\\ b_{64}^0\n",
    "  \\end{smallmatrix}\\right]\n",
    "  \\right)\n",
    "  \\\\\n",
    "  \\hat{y}\n",
    "  &=\n",
    "  \\operatorname{logistic}\n",
    "  \\left(\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  W_{1}^1 \\\\\n",
    "  \\vdots \\\\\n",
    "  W_{64}^1 \\\\\n",
    "  \\end{smallmatrix}\\right]^\\mathsf{T}\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  z_1^1 \\\\ \\vdots \\\\ z_{64}^1\n",
    "  \\end{smallmatrix}\\right]\n",
    "  +\n",
    "  b^1\n",
    "  \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Don't forget the transpose symbol in the equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are computing the predictions of $N$ images at once, then we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  z_1^{1, (1)} & \\cdots & z_{64}^{1, (1)} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  z_1^{1, (N)} & \\cdots & z_{64}^{1, (N)} \\\\ \n",
    "  \\end{smallmatrix}\\right]\n",
    "  &=\n",
    "  \\operatorname{logistic}\n",
    "  \\left(\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  x_1^{(1)} & \\cdots & x_{16384}^{(1)} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  x_1^{(N)} & \\cdots & x_{16384}^{(N)} \\\\\n",
    "  \\end{smallmatrix}\\right]\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  W_{1,1}^0 & \\cdots & W_{1,64}^0 \\\\\n",
    "  \\vdots    & \\ddots & \\vdots \\\\\n",
    "  W_{16384,1}^0 & \\cdots & W_{16384,64}^0 \\\\\n",
    "  \\end{smallmatrix}\\right]\n",
    "  +\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  b_1^0  & \\cdots & b_{64}^0 \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  b_1^0  & \\cdots & b_{64}^0 \\\\\n",
    "  \\end{smallmatrix}\\right]\n",
    "  \\right)\n",
    "  \\\\\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  \\hat{y}^{(1)} \\\\ \\vdots \\\\ \\hat{y}^{(N)}\n",
    "  \\end{smallmatrix}\\right]\n",
    "  &=\n",
    "  \\operatorname{logistic}\n",
    "  \\left(\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  z_1^{1, (1)} & \\cdots & z_{64}^{1, (1)} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  z_1^{1, (N)} & \\cdots & z_{64}^{1, (N)} \\\\ \n",
    "  \\end{smallmatrix}\\right]\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  W_{1}^1 \\\\\n",
    "  \\vdots \\\\\n",
    "  W_{64}^1 \\\\\n",
    "  \\end{smallmatrix}\\right]\n",
    "  +\n",
    "  \\left[\\begin{smallmatrix}\n",
    "  b^1 \\\\\n",
    "  \\vdots \\\\\n",
    "  b^1\n",
    "  \\end{smallmatrix}\\right]\n",
    "  \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "You may notice that we use $W^\\mathsf{T}\\mathbf{x}$ when we have a single image and use $XW$ when we have multiple images ($N$ images in this case). This is because we use a column vector to describe a single image, but we use a matrix to represent multiple images, in which each row vector represents one image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are confused about these linear algebra calculations, writing down the shapes of these vectors and matrices may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write the code for our neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(x, params):\n",
    "    \"\"\"A fully-connected neural network with L=1.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x : numpy.ndarray\n",
    "        The input of the model. It's shape should be (n_images, n_total_pixels).\n",
    "    params : a tuple/list of four elements\n",
    "        - The first element is W0, a 2D array with shape (n_total_pixels, n_z1).\n",
    "        - The second elenment is b0, an 1D array with length n_z1.\n",
    "        - The third element is W1, an 1D array with length n_z1.\n",
    "        - The fourth element is b1, a scalar.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    yhat : numpy.ndarray\n",
    "        The predicted values obtained from the model. It's an 1D array with\n",
    "        length n_images.\n",
    "    \"\"\"\n",
    "    z1 = logistic(numpy.dot(x, params[0])+params[1])\n",
    "    yhat = logistic(numpy.dot(z1, params[2])+params[3])\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is the same as in lesson 5, except we now consider both $W^0$ and $W^1$ in the regularization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(x, true_labels, params):\n",
    "    \"\"\"Calculate the predictions and the loss w.r.t. the true values.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x : numpy.ndarray\n",
    "        The input of the model. The shape should be (n_images, n_total_pixels).\n",
    "    true_labels : numpy.ndarray\n",
    "        The true labels of the input images. Should be 1D and have length of\n",
    "        n_images.\n",
    "    params : a tuple/list of two elements\n",
    "        - The first element is W0, a 2D array with shape (n_total_pixels, n_z1).\n",
    "        - The second elenment is b0, an 1D array with length n_z1.\n",
    "        - The third element is W1, an 1D array with length n_z1.\n",
    "        - The fourth element is b1, a scalar.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss : a scalar\n",
    "        The summed loss.\n",
    "    \"\"\"\n",
    "    pred = neural_network_model(x, params)\n",
    "    \n",
    "    n_images = x.shape[0]\n",
    "    \n",
    "    # major loss\n",
    "    loss = - (\n",
    "        numpy.dot(true_labels, numpy.log(pred+1e-15)) +\n",
    "        numpy.dot(1.-true_labels, numpy.log(1.-pred+1e-15))\n",
    "    ) / n_images\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_loss(x, true_labels, params, _lambda=1.):\n",
    "    \"\"\"Return the loss with regularization.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x, true_labels, params :\n",
    "        Parameters for function `model_loss`.\n",
    "    _lambda : float\n",
    "        The weight of the regularization term. Default: 0.01\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss : a scalar\n",
    "        The summed loss.\n",
    "    \"\"\"\n",
    "    loss = model_loss(x, true_labels, params)\n",
    "    Nw = params[0].shape[0] * params[0].shape[1] + params[2].size\n",
    "    reg = ((params[0]**2).sum() + (params[2]**2).sum()) / Nw\n",
    "    return loss + _lambda * reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we rely on the `grad` from `autograd` to get a function that calculates the derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get the gradients of a logistic model\n",
    "gradients = grad(regularized_loss, argnum=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our convenience, we use variables `n0` to store the length of input vector $\\mathbf{x}$ (recall that $\\mathbf{z}^0\\equiv\\mathbf{x}$) and use `n1` to represent $n_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of elements in z0 (i.e., x), z1, ...\n",
    "n0 = images_train.shape[1]  # i.e., res * res\n",
    "n1 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our regular weight initialization is not working!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we should initialize the parameters with zeros, just like what we did in previous lessons. However, this is not going to work for neural networks when $L \\gt 0$. We will show you why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "W0 = numpy.zeros((n0, n1))\n",
    "b0 = numpy.zeros(n1)\n",
    "W1 = numpy.zeros(n1)\n",
    "b1 = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have all-zero elements in parameter matrices, the gradients of $W^i$ for $i \\ne L$ will be zeros. In our model ($L=1$), that says the gradient of $W^0$ will be zero. Without dragging you into messy mathematical equations, we can show it by using the `gradients` function to calculate the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of W0 are zeros: True\n",
      "Gradients of W1 are zeros: False\n"
     ]
    }
   ],
   "source": [
    "grads = gradients(images_train, labels_train, (W0, b0, W1, b1))\n",
    "print(\"Gradients of W0 are zeros:\", numpy.allclose(grads[0], 0.))\n",
    "print(\"Gradients of W1 are zeros:\", numpy.allclose(grads[2], 0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how does this issue matter? Recall how we perform gradient descent -- we update parameters by substracting gradients from the current values of parameters:\n",
    "\n",
    "$$\n",
    "\\text{new }W^0 = \\text{current }W^0 - \\text{step size}\\times\\text{gradients of }W^0\n",
    "$$\n",
    "\n",
    "So if the gradients of $W^0$ are zeros, then gradient descent will not change the values of $W^0$ at all. In other words, our model will not improve no matter how many iterations we run for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier/Glorot initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve the issue, we need non-zero initial values for parameters. Xavier initialization (or sometimes called Glorot initialization) is the most common method to initialize parameters. The mathematical proof of why it works under the hood requires knowledge in statistics and probabilities. You can check reference [1] if you're interested, but here we focus on how to do it instead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W^i = \\text{random numbers uniformly drawn from interval }\\left[-\\sqrt{\\frac{6}{n_i+n_{i+1}}}, \\sqrt{\\frac{6}{n_i+n_{i+1}}}\\right]\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathbf{b}^i = \\mathbf{0}\n",
    "$$\n",
    "where the bold-face zero, $\\mathbf{0}$, denotes a vector of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "scale = numpy.sqrt(6/(n0+n1))\n",
    "W0 = numpy.random.uniform(-scale, scale, (n0, n1))\n",
    "b0 = numpy.zeros(n1)\n",
    "\n",
    "scale = numpy.sqrt(6/(n1+1))\n",
    "W1 = numpy.random.uniform(-scale, scale, n1)\n",
    "b1 = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you check the gradients of $W^0$ using the function `gradients`, they are not zeros anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a look at the initial performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial precision: 59.2%\n",
      "Initial recall: 88.5%\n",
      "Initial F-score: 71.0%\n"
     ]
    }
   ],
   "source": [
    "# initial accuracy\n",
    "pred_labels_test = classify(images_test, (W0, b0, W1, b1), neural_network_model)\n",
    "perf = performance(pred_labels_test, labels_test)\n",
    "print(\"Initial precision: {:.1f}%\".format(perf[0]*100))\n",
    "print(\"Initial recall: {:.1f}%\".format(perf[1]*100))\n",
    "print(\"Initial F-score: {:.1f}%\".format(perf[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization is the same as how we did it in lesson 5, except that now we have to update `W0`, `b0`, `W1`, and `b1` (in lesson 5, we only updated `W` and `b`). Also, we give you a proper step size (`lr`) directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10...20...30...40...50...60...70...80...90...100...110...120...130...140...150...160...170...180...190...200...210...220...230...240...250...260...270...280...290...300...310...320...330...340...350...360...370...380...390...400...410...420...430...440...450...CPU times: user 3min, sys: 9.24 s, total: 3min 9s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# step size\n",
    "lr = 1e-1\n",
    "\n",
    "# a variable for the change in validation loss\n",
    "change = numpy.inf\n",
    "\n",
    "# a counter for optimization iterations\n",
    "i = 0\n",
    "\n",
    "# a variable to store the validation loss from the previous iteration\n",
    "old_val_loss = 1e-15\n",
    "\n",
    "# keep running if:\n",
    "#   1. we still see significant changes in validation loss\n",
    "#   2. iteration counter < 10000\n",
    "while change >= 1e-6 and i < 10000:\n",
    "    \n",
    "    # calculate gradients and use gradient descents\n",
    "    grads = gradients(images_train, labels_train, (W0, b0, W1, b1))\n",
    "    W0 -= (grads[0] * lr)\n",
    "    b0 -= (grads[1] * lr)\n",
    "    W1 -= (grads[2] * lr)\n",
    "    b1 -= (grads[3] * lr)\n",
    "    \n",
    "    # validation loss\n",
    "    val_loss = model_loss(images_val, labels_val, (W0, b0, W1, b1))\n",
    "    \n",
    "    # calculate f-scores against the validation dataset\n",
    "    pred_labels_val = classify(images_val, (W0, b0, W1, b1), neural_network_model)\n",
    "    score = performance(pred_labels_val, labels_val)\n",
    "\n",
    "    # calculate the chage in validation loss\n",
    "    change = numpy.abs((val_loss-old_val_loss)/old_val_loss)\n",
    "\n",
    "    # update the counter and old_val_loss\n",
    "    i += 1\n",
    "    old_val_loss = val_loss\n",
    "    \n",
    "    # print the progress every 10 steps\n",
    "    if i % 10 == 0:\n",
    "        print(\"{}...\".format(i), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Upon optimization stopped:\n",
      "    Iterations: 453\n",
      "    Validation loss w/o regularization: 0.2787043104606595\n",
      "    Validation loss w/ regularization: 0.024497566748684113\n",
      "    Validation precision: 0.8993288590604027\n",
      "    Validation recall: 0.8589743589743589\n",
      "    Validation F-score: 0.8786885245901639\n",
      "    Change in validation loss: 1.6184802410438597e-07\n"
     ]
    }
   ],
   "source": [
    "val_loss_reg = regularized_loss(images_train, labels_train, (W0, b0, W1, b1))\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Upon optimization stopped:\")\n",
    "print(\"    Iterations:\", i)\n",
    "print(\"    Validation loss w/o regularization:\", val_loss)\n",
    "print(\"    Validation loss w/ regularization:\", val_loss_reg)\n",
    "print(\"    Validation precision:\", score[0])\n",
    "print(\"    Validation recall:\", score[1])\n",
    "print(\"    Validation F-score:\", score[2])\n",
    "print(\"    Change in validation loss:\", change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the final model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final precision: 91.8%\n",
      "Final recall: 93.6%\n",
      "Final F-score: 92.7%\n"
     ]
    }
   ],
   "source": [
    "# final accuracy\n",
    "pred_labels_test = classify(images_test, (W0, b0, W1, b1), neural_network_model)\n",
    "perf = performance(pred_labels_test, labels_test)\n",
    "print(\"Final precision: {:.1f}%\".format(perf[0]*100))\n",
    "print(\"Final recall: {:.1f}%\".format(perf[1]*100))\n",
    "print(\"Final F-score: {:.1f}%\".format(perf[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Compared to lesson 5, simply replacing the logistic regression model with a fully connected neural network model with $L=1$ improves the F-score from $86.6\\%$ to $92.7\\%$. The precision is now $91.8\\%$, meaning whenever our model predicts a casting part being defective, $91.8\\%$ chance it's correct. And the recall is now $93.4\\%$, which says that our model misses around $6.6\\%$ of defective parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the performance may still be unsatisfying. However, one benefit of using neural networks is that we can control the complexity of our model. For example, we may get better performance if we increase $L$ and the lengths of intermediate results $\\mathbf{z}^1$, $\\mathbf{z}^2$, $\\dots$, $\\mathbf{z}^L$. \n",
    "\n",
    "What we didn't address in this lesson, however, is that increasing the complexity increases the difficulty of optimization as well. Optimization may need much more time to converge or may not even find a satisfying solution. So it's not like we can freely increase $L$ and the lengths of intermediate results to whatever values we like. One way to ease the optimization when having high $L$ and lengths is to use advanced optimization methods. In lesson 8, we will see more different optimization methods. Another approach is to use other types of neural networks, such as a convolutional neural network, which is a more popular choice for computer vision applications. We will talk about this in a later lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lesson, we learned:\n",
    "\n",
    "1. the concept of fully connected neural networks,\n",
    "2. how to code a fully connected neural network, and\n",
    "3. how to initialize parameters of a neural network.\n",
    "\n",
    "We hope this lesson gave you some sense of what neural networks and deep learning are and how they are the same as or different from linear/logistic regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In Y. W. Teh & M. Titterington (Eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (Vol. 9, pp. 249–256). PMLR. http://proceedings.mlr.press/v9/glorot10a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"https://fonts.googleapis.com/css?family=Merriweather:300,300i,400,400i,700,700i,900,900i\" rel='stylesheet' >\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,400,400i,700,700i\" rel='stylesheet' >\n",
       "<link href='https://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' >\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.cell { /* set cell width */\n",
       "    width: 800px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    width: 1000px;\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "margin-top:0.5em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running { \n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256); \n",
       "    border-radius: 0px; \n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Source Sans Pro', sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 110%;\n",
       "    width:680px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Merriweather', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: bold;    \n",
       "    font-size: 250%;\n",
       "    line-height: 100%;\n",
       "    color: #004065;\n",
       "    margin-bottom: 1em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\t\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Merriweather', serif;\n",
       "    font-weight: bold; \n",
       "    font-size: 180%;\n",
       "    line-height: 100%;\n",
       "    color: #0096d6;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\t\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Merriweather', serif;\n",
       "\tfont-size: 150%;\n",
       "    margin-top:12px;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: #008367;\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Merriweather', serif;\n",
       "    font-weight: 300; \n",
       "    font-size: 100%;\n",
       "    line-height: 120%;\n",
       "    text-align: left;\n",
       "    width:500px;\n",
       "    margin-top: 1em;\n",
       "    margin-bottom: 2em;\n",
       "    margin-left: 80pt;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Source Sans Pro', sans-serif;\n",
       "    font-weight: regular;\n",
       "    font-size: 130%;\n",
       "    color: #e31937;\n",
       "    font-style: italic;\n",
       "    margin-bottom: .5em;\n",
       "    margin-top: 1em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'Source Code Pro', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "\t\n",
       "    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"], \n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
