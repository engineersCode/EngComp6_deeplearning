{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43752c9",
   "metadata": {},
   "source": [
    "###### Content under Creative Commons Attribution license CC-BY 4.0, code under BSD 3-Clause License Â© 2021 Lorena A. Barba, Tingyu Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b27e0",
   "metadata": {},
   "source": [
    "# Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d0727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from autograd import grad\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738101c6",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "Let's generate some synthetic data using a polynomial function of $y = x^4 + x^3 - 4x^2 $ with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data\n",
    "np.random.seed(5)\n",
    "x = np.linspace(-3, 3, 20)\n",
    "y = x**4 + x**3 - 4*x**2 + 8*np.random.normal(size=len(x))\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eaaaf",
   "metadata": {},
   "source": [
    "Suppose now we are only given the data (not the function that generates them), and our goal is to curve fit the data. The nonlinear relationship between $x$ and $y$ suggests that a linear regression will fail. Intuitively, nonlinear models should be a good fit, and among them, regressions using polynomial functions may first come to your mind.\n",
    "\n",
    "Let's write this basic model as the $d$th-order polynomial of $x$, $x$ being the only feature here:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\cdots + w_d x^d, \n",
    "$$\n",
    "\n",
    "where $w$ denotes the weights. Keep in mind that in the model fitting context, the objective is always finding the optimal values of these weights given $x$ and $y$. When viewed from a different perspective, the model above is just a linear combination of the weights. In fact, by creating polynomial features of $x$, namely, letting $x_i = x^i$, the model becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_d x_d,\n",
    "$$\n",
    "\n",
    "a carbon copy of the multiple linear regression (MLR) model. The matrix form is also $\\hat{\\mathbf{y}} = X\\mathbf{w}$, and the only gap here is to form the matrix $X$ using the power of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ffa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 3\n",
    "\n",
    "def polynomial_features(x, degree):\n",
    "    X = np.empty((len(x), degree+1))\n",
    "    for i in range(degree+1):\n",
    "        X[:,i] = x**i\n",
    "    return X\n",
    "\n",
    "X = polynomial_features(x, degree)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38695be0",
   "metadata": {},
   "source": [
    "Unsurprisingly, scikit-learn offers a counterpart: `PolynomialFeatures()`, which we will use later:\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree, include_bias=True)\n",
    "X = poly.fit_transform(x.reshape(-1,1))\n",
    "```\n",
    "\n",
    "Next, recall that for MLR, we should normalize each feature to a same scale, except for the first column, because $x_0$ is set to 1 for all entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "X_scaled[:,0] = 1   # the column for intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a0ed7",
   "metadata": {},
   "source": [
    "Let's reuse the same model and loss functions,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cec129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(params, X):\n",
    "    \"\"\"\n",
    "    The linear regression model in matrix form.\n",
    "    \"\"\"\n",
    "    return np.dot(X, params)\n",
    "\n",
    "def mse_loss(params, model, X, y):\n",
    "    \"\"\"\n",
    "    The mean squared error loss function.\n",
    "    \"\"\"\n",
    "    y_pred = model(params, X)\n",
    "    return np.mean( np.sum((y-y_pred)**2) )\n",
    "\n",
    "gradient = grad(mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03085e",
   "metadata": {},
   "source": [
    "and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8665e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 3000\n",
    "alpha = 0.01\n",
    "params = np.zeros(X_scaled.shape[1])\n",
    "descent = np.ones(X_scaled.shape[1])\n",
    "i = 0\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "while np.linalg.norm(descent) > 0.01 and i < max_iter:\n",
    "    descent = gradient(params, linear_regression, X_scaled, y)\n",
    "    params = params - descent * alpha\n",
    "    loss = mse_loss(params, linear_regression, X_scaled, y)\n",
    "    mae = mean_absolute_error(y, X_scaled@params)\n",
    "    if i%100 == 0:\n",
    "        print(f\"iteration {i:4}, {loss = :.3f}, {mae = :.3f}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a4680",
   "metadata": {},
   "source": [
    "To plot the fitted curve, we need to generate a grid `xgrid` along $x$-axis, and predict $y$ at these locations. But before making the predictions directly by multiplying the weights `params`, don't forget to repeat the procedure of how we prepared $X$ - creating polynomial features and normalizing them - on `xgrid`. Ponder for a moment to get your mind around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrid = np.linspace(x.min(), x.max(), 30)\n",
    "xgrid_poly_feat = polynomial_features(xgrid, degree)\n",
    "xgrid_scaled = min_max_scaler.transform(xgrid_poly_feat)\n",
    "xgrid_scaled[:,0] = 1 \n",
    "plt.scatter(x, y, c='r', label='observed')\n",
    "plt.plot(xgrid, xgrid_scaled@params, label='predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb96f5",
   "metadata": {},
   "source": [
    "## Observe underfitting & overfitting\n",
    "\n",
    "In the model above, we just randomly picked a polynomial degree of $3$ out of our mind. However, it is good or complicated enough to model our dataset? In other words, should we try higher orders?\n",
    "\n",
    "Let's repeat our study with different degrees varying from 1 to 15, and plot these curves interactively using `ipywidget`.\n",
    "To faciliate our experiment, we use `make_pipeline` from scikit-learn to combine our 3-step workflow: creating polynomial features, features scaling and linear regression into one step. The rest of the code is for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4dd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def polyreg_helper(degree, x, y):\n",
    "    \"\"\"\n",
    "    The helper function to plot polynomial linear regression.\n",
    "    \n",
    "    Args:\n",
    "        degree: Polynomial degree.\n",
    "        x,y: Training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_plot = np.linspace(x.min(), x.max(), 30).reshape(-1,1)\n",
    "    \n",
    "    linear = make_pipeline(PolynomialFeatures(degree, include_bias=False),\n",
    "                           MinMaxScaler(),\n",
    "                           LinearRegression(fit_intercept=True))\n",
    "    linear.fit(x.reshape(-1,1), y)\n",
    "    mae = mean_absolute_error(y, linear.predict(x.reshape(-1,1)))\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.scatter(x, y, c='r', label='observed')\n",
    "    ax.plot(x_plot, linear.predict(x_plot), label='predicted')\n",
    "    ax.set_title(f\"Polynomial degree = {degree:2}, MAE = {mae:.3f}\", fontsize=16)\n",
    "    ax.legend()\n",
    "    plt.subplots_adjust(top=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed9dd71",
   "metadata": {},
   "source": [
    "Drag the slider below to visualize the fitted model using different polynomial orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25238cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, fixed\n",
    "degree_slider = widgets.IntSlider(min=1, max=15, step=1)\n",
    "widgets.interact(polyreg_helper, degree=degree_slider, x=fixed(x), y=fixed(y));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc128660",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "\n",
    "When `degree` is $1$, the straightline clearly fails to capture the underlying relationship between $x$ and $y$. Specifically, the model is too simple to explain the **variance** in the data, and is thus regarded **underfitting**. With a degree of $2$, a quadratic curve still fails to \"connect\" some of the points.\n",
    "\n",
    "Another term for underfitting is **high bias**. You can think of that with a degree of $1$, we are biased to assume a linear relationship between $x$ and $y$.\n",
    "\n",
    "> Question: Would having more training data help resolve underfitting?\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "As we increase the polynomial degree, the training error (MAE from the figure title) keep decreasing. \n",
    "Does it mean the curve with the highest degree ($15$) fits the best?\n",
    "Maybe, but probably not.\n",
    "Drag the slider to the right, we will find that fitted curve passes exactly through many points and looks very strange.\n",
    "If we are given new data, this model would be unable to generalize because it fits too closely to the old data.\n",
    "At this point, the model is just too flexible and it fits the noise rather than the true relationship. \n",
    "In this case, the model is **overfitting**, or has a **high variance**.\n",
    "\n",
    "Overfitting usually happens when you have a large number of features (ex. degree of polynomial in this lesson) but you don't have enough data to constrain them. Common ways to avoid overfitting include:\n",
    "\n",
    "- limit the number of features\n",
    "- model selection\n",
    "- regularization\n",
    "\n",
    "This first method is problem-specific and usually requires you to decide which features to keep. Later in this module, we will discuss how to select from models by splitting our data into train and test sets. Now let's move on to regularization in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e976c554",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "The idea of regularization is straighforward: to keep all features by introducing a term in the loss function that penalizes complicated models. In our polynomial regression model:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\cdots + w_d x^d,\n",
    "$$\n",
    "\n",
    "every polynomial term, even the intercept term $w_0$, contributes to the overall complexity of the model.\n",
    "A simple idea is to add constraints to the magnitudes of these weights.\n",
    "\n",
    "The MSE loss can be written in matrix form, as a function of $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w})=\\frac{1}{N} {\\lVert \\mathbf{y} - X\\mathbf{w} \\rVert}^2 \n",
    "$$\n",
    "\n",
    "By adding a regularization term (in $L_2$-norm) ${\\lVert \\mathbf{w} \\rVert}^2 = \\sum_{j=0}^d w_j^2$ to the loss, the new loss now favors smaller weights. As weights are closer to zero, the model tends to be simpler, thus is less likely to overfit. The matrix form of the regularized loss is:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w})=\\frac{1}{N} {\\lVert \\mathbf{y} - X\\mathbf{w} \\rVert}^2 + \\lambda {\\lVert \\mathbf{w} \\rVert}^2,\n",
    "$$\n",
    "\n",
    "where $\\lambda$ denotes the regularization parameter that controls the tradeoff between fitting the data well (the first term) and keeping the model simple to avoid overfitting (the second term).\n",
    "\n",
    "Let's code the regularized MSE loss.\n",
    "\n",
    "**to do**:\n",
    "\n",
    "- clarify that we don't penalize the bias term $w_0$, need to rewrite the equations\n",
    "- shall we mention the tradeoff between model error and bias error (variance-bias tradeoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_loss(params, model, X, y, _lambda=1.0):\n",
    "    \"\"\"\n",
    "    The mean squared error loss function with an L2 penalty.\n",
    "    \"\"\"\n",
    "    y_pred = model(params, X)\n",
    "    return np.mean( np.sum((y-y_pred)**2) ) + _lambda * np.sum( params[1:]**2 )\n",
    "\n",
    "gradient = grad(regularized_loss)                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_regularization_params = params.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bd1b5",
   "metadata": {},
   "source": [
    "And train the model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f48e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 3000\n",
    "alpha = 0.01\n",
    "params = np.zeros(X_scaled.shape[1])\n",
    "descent = np.ones(X_scaled.shape[1])\n",
    "i = 0\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "while np.linalg.norm(descent) > 0.01 and i < max_iter:\n",
    "    descent = gradient(params, linear_regression, X_scaled, y)\n",
    "    params = params - descent * alpha\n",
    "    loss = mse_loss(params, linear_regression, X_scaled, y)\n",
    "    mae = mean_absolute_error(y, X_scaled@params)\n",
    "    if i%100 == 0:\n",
    "        print(f\"iteration {i:4}, {loss = :.3f}, {mae = }\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e0401",
   "metadata": {},
   "source": [
    "With `degree` set to $3$, let's compare the optimal weights before and after regularization. We can reuse the `xgrid` to plot both curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3428b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"weights without regularization\")\n",
    "print(no_regularization_params)\n",
    "print(\"weights with regularization\")\n",
    "print(params)\n",
    "\n",
    "plt.scatter(x, y, c='r')\n",
    "plt.plot(xgrid, xgrid_scaled@no_regularization_params, label='w/o regularization')\n",
    "plt.plot(xgrid, xgrid_scaled@params, label='with regularization')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb289458",
   "metadata": {},
   "source": [
    "There are many choices of regularization. The one we showed here uses the $L_2$-norm of the weights.\n",
    "\n",
    "Again, let's plot both models with varying polynomial degree using ipywidget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyreg_regularization_helper(degree, x, y):\n",
    "    \"\"\"\n",
    "    The helper function to plot polynomial linear regression with regularization.\n",
    "    \n",
    "    Args:\n",
    "        degree: Polynomial degree.\n",
    "        x,y: Training data.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import Ridge\n",
    "    \n",
    "    x_plot = np.linspace(x.min(), x.max(), 30).reshape(-1,1)\n",
    "    \n",
    "    linear = make_pipeline(PolynomialFeatures(degree, include_bias=False),\n",
    "                           MinMaxScaler(),\n",
    "                           LinearRegression())\n",
    "    linear.fit(x.reshape(-1,1), y)\n",
    "    mae_linear = mean_absolute_error(y, linear.predict(x.reshape(-1,1)))\n",
    "    \n",
    "    ridge = make_pipeline(PolynomialFeatures(degree, include_bias=False),\n",
    "                          MinMaxScaler(),\n",
    "                          Ridge(alpha=1.0))\n",
    "    ridge.fit(x.reshape(-1,1), y)\n",
    "    mae_ridge = mean_absolute_error(y, ridge.predict(x.reshape(-1,1)))\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.scatter(x, y, c='r', label='observed')\n",
    "    ax.plot(x_plot, linear.predict(x_plot), label='w/o regularization, predicted')\n",
    "    ax.plot(x_plot, ridge.predict(x_plot), label='with regularization, predicted')\n",
    "    ax.set_title(f\"Poly degree = {degree:2}, MAE_linear = {mae_linear:.3f}, MAE_ridge = {mae_ridge:.3f}\", fontsize=16)\n",
    "    ax.legend()\n",
    "    plt.subplots_adjust(top=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82946946",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "widgets.interact(polyreg_regularization_helper,\n",
    "                 degree=degree_slider, x=fixed(x), y=fixed(y));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18864d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to load the notebook's style sheet, then ignore it\n",
    "from IPython.core.display import HTML\n",
    "css_file = '../style/custom.css'\n",
    "HTML(open(css_file, \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3828f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
